{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":"<p>Click here if you are not redirected automatically</p>"},{"location":"core-concepts/classification/","title":"Document Classification","text":"<p>In document intelligence, classification is often the crucial first step. It sets the stage for subsequent processes like data extraction and analysis. Before the rise of LLMs, this used to be accomplished (and still is) with AI models training in-house for certain use cases. Services such as Azure Document Intelligence give you this feature, but they are not dynamic and will set you up for \"Vendor lock-in\".</p> <p>LLMs may not be the most efficient for this task, but they are agnostic and near-perfect for it.</p>"},{"location":"core-concepts/classification/#classification-techniques","title":"Classification Techniques","text":"<ul> <li> <p> Basic Classification</p> <p>Simple yet powerful classification using a single LLM with contract mapping.</p> <p> Learn More</p> </li> <li> <p> Mixture of Models (MoM)</p> <p>Enhance accuracy by combining multiple models with different strategies.</p> <p> Learn More</p> </li> <li> <p> Tree-Based Classification</p> <p>Handle complex hierarchies and similar document types efficiently.</p> <p> Learn More</p> </li> <li> <p> Vision Classification</p> <p>Leverage visual features for better accuracy.</p> <p> Learn More</p> </li> </ul>"},{"location":"core-concepts/classification/#classification-response","title":"Classification Response","text":"<p>All classification methods return a standardized response:</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\nclass ClassificationResponse(BaseModel):\n    name: str\n    confidence: Optional[int] = Field(\n        description=\"From 1 to 10. 10 being the highest confidence\",\n        ge=1, \n        le=10\n    )\n</code></pre>"},{"location":"core-concepts/classification/#available-strategies","title":"Available Strategies","text":"<p>ExtractThinker supports three main classification strategies:</p> <ul> <li>CONSENSUS: All models must agree on the classification</li> <li>HIGHER_ORDER: Uses the result with highest confidence</li> <li>CONSENSUS_WITH_THRESHOLD: Requires consensus and minimum confidence</li> </ul> <p>For detailed implementation of each technique, visit their respective pages.</p>"},{"location":"core-concepts/classification/basic/","title":"Basic Classification","text":"<p>When classifying documents, the process involves extracting the content of the document and adding it to the prompt with several possible classifications. ExtractThinker simplifies this process using Pydantic models and instructor.</p>"},{"location":"core-concepts/classification/basic/#simple-classification","title":"Simple Classification","text":"<p>The most straightforward way to classify documents:</p> <pre><code>from extract_thinker import Classification, Extractor\nfrom extract_thinker.document_loader import DocumentLoaderTesseract\n\n# Define classifications\nclassifications = [\n    Classification(\n        name=\"Driver License\",\n        description=\"This is a driver license\",\n        contract=DriverLicense,  # optional. Will be added to the prompt\n    ),\n    Classification(\n        name=\"Invoice\",\n        description=\"This is an invoice\",\n        contract=InvoiceContract,  # optional. Will be added to the prompt\n    ),\n]\n\n# Initialize extractor\ntesseract_path = os.getenv(\"TESSERACT_PATH\")\ndocument_loader = DocumentLoaderTesseract(tesseract_path)\nextractor = Extractor(document_loader)\nextractor.load_llm(\"gpt-4o\")\n\n# Classify document\nresult = extractor.classify(INVOICE_FILE_PATH, classifications)\nprint(f\"Document type: {result.name}, Confidence: {result.confidence}\")\n</code></pre>"},{"location":"core-concepts/classification/basic/#type-mapping-with-contract","title":"Type Mapping with Contract","text":"<p>Adding contract structure to the classification improves accuracy:</p> <pre><code>from typing import List\nfrom extract_thinker.models.contract import Contract\n\nclass InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    lines: List[LineItem]\n    total_amount: float\n\nclass DriverLicense(Contract):\n    name: str\n    age: int\n    license_number: str\n</code></pre> <p>The contract structure is automatically added to the prompt, helping the model understand the expected document structure.</p>"},{"location":"core-concepts/classification/basic/#classification-response","title":"Classification Response","text":"<p>All classifications return a standardized response:</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\nclass ClassificationResponse(BaseModel):\n    name: str\n    confidence: Optional[int] = Field(\n        description=\"From 1 to 10. 10 being the highest confidence\",\n        ge=1, \n        le=10\n    )\n</code></pre>"},{"location":"core-concepts/classification/basic/#best-practices","title":"Best Practices","text":"<ul> <li>Provide clear, distinctive descriptions for each classification</li> <li>Use contract structures when possible</li> <li>Consider using image classification for visual documents</li> <li>Monitor confidence scores</li> <li>Handle low-confidence cases appropriately</li> </ul> <p>For more advanced classification techniques, see: - Mixture of Models (MoM) - Tree-Based Classification - Vision Classification </p>"},{"location":"core-concepts/classification/mom/","title":"Mixture of Models (MoM)","text":"<p>The Mixture of Models (MoM) is a pattern that increases classification confidence by combining multiple models in parallel. This approach is particularly effective when using instructor for structured outputs.</p>"},{"location":"core-concepts/classification/mom/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import Process, Classification, ClassificationStrategy\nfrom extract_thinker.document_loader import DocumentLoaderTesseract\n\n# Define classifications\nclassifications = [\n    Classification(\n        name=\"Driver License\",\n        description=\"This is a driver license\",\n    ),\n    Classification(\n        name=\"Invoice\",\n        description=\"This is an invoice\",\n    ),\n]\n\n# Initialize document loader\ntesseract_path = os.getenv(\"TESSERACT_PATH\")\ndocument_loader = DocumentLoaderTesseract(tesseract_path)\n\n# Initialize multiple extractors with different models\ngpt_35_extractor = Extractor(document_loader)\ngpt_35_extractor.load_llm(\"gpt-3.5-turbo\")\n\nclaude_extractor = Extractor(document_loader)\nclaude_extractor.load_llm(\"claude-3-haiku-20240307\")\n\ngpt4_extractor = Extractor(document_loader)\ngpt4_extractor.load_llm(\"gpt-4o\")\n\n# Create process with multiple extractors\nprocess = Process()\nprocess.add_classify_extractor([\n    [gpt_35_extractor, claude_3_haiku_extractor],  # First layer\n    [gpt4_extractor],                              # Second layer\n])\n\n# Classify with consensus strategy\nresult = process.classify(\n    \"document.pdf\",\n    classifications,\n    strategy=ClassificationStrategy.CONSENSUS_WITH_THRESHOLD,\n    threshold=9\n)\n</code></pre>"},{"location":"core-concepts/classification/mom/#available-strategies","title":"Available Strategies","text":""},{"location":"core-concepts/classification/mom/#consensus","title":"CONSENSUS","text":"<p>All models must agree on the classification:</p> <pre><code>result = process.classify(\n    \"document.pdf\",\n    classifications,\n    strategy=ClassificationStrategy.CONSENSUS\n)\n</code></pre>"},{"location":"core-concepts/classification/mom/#higher_order","title":"HIGHER_ORDER","text":"<p>Uses the result with the highest confidence score:</p> <pre><code>result = process.classify(\n    \"document.pdf\",\n    classifications,\n    strategy=ClassificationStrategy.HIGHER_ORDER\n)\n</code></pre>"},{"location":"core-concepts/classification/mom/#consensus_with_threshold","title":"CONSENSUS_WITH_THRESHOLD","text":"<p>Requires both consensus and minimum confidence:</p> <pre><code>result = process.classify(\n    \"document.pdf\",\n    classifications,\n    strategy=ClassificationStrategy.CONSENSUS_WITH_THRESHOLD,\n    threshold=9\n)\n</code></pre>"},{"location":"core-concepts/classification/mom/#best-practices","title":"Best Practices","text":"<ul> <li>Use smaller models in the first layer for cost efficiency</li> <li>Reserve larger models for cases where consensus isn't reached</li> <li>Set appropriate confidence thresholds based on your use case</li> <li>Consider using different model providers for better diversity</li> <li>Monitor and log classification results for each model</li> </ul>"},{"location":"core-concepts/classification/tree/","title":"Tree-Based Classification","text":"<p>In document intelligence, challenges often arise when dealing with a large number of similar document types. Tree-based classification organizes classifications into a hierarchical structure, breaking down the task into smaller, more manageable batches.</p>"},{"location":"core-concepts/classification/tree/#basic-concept","title":"Basic Concept","text":"<p>Tree-based classification offers: - Increased Accuracy: By narrowing down options at each step - Scalability: Easy addition of new document types - Reduced Context: Smaller context windows at each level</p>"},{"location":"core-concepts/classification/tree/#implementation","title":"Implementation","text":"<p>Here's how to implement a classification tree:</p> <pre><code>from extract_thinker import Classification, ClassificationNode, ClassificationTree\nfrom extract_thinker.models.contract import Contract\n\n# Define contracts for each level\nclass FinancialContract(Contract):\n    total_amount: int\n    document_number: str\n    document_date: str\n\nclass InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    lines: List[LineItem]\n    total_amount: float\n\nclass CreditNoteContract(Contract):\n    credit_note_number: str\n    credit_note_date: str\n    lines: List[LineItem]\n    total_amount: float\n\n# Create the classification tree\nfinancial_docs = ClassificationNode(\n    classification=Classification(\n        name=\"Financial Documents\",\n        description=\"This is a financial document\",\n        contract=FinancialContract,\n    ),\n    children=[\n        ClassificationNode(\n            classification=Classification(\n                name=\"Invoice\",\n                description=\"This is an invoice\",\n                contract=InvoiceContract,\n            )\n        ),\n        ClassificationNode(\n            classification=Classification(\n                name=\"Credit Note\",\n                description=\"This is a credit note\",\n                contract=CreditNoteContract,\n            )\n        )\n    ]\n)\n\n# Create the tree\nclassification_tree = ClassificationTree(\n    nodes=[financial_docs]\n)\n\n# Initialize process\nprocess = Process()\nprocess.add_classify_extractor([[extractor]])\n\n# Classify using tree\nresult = process.classify(\n    \"document.pdf\",\n    classification_tree,\n    threshold=0.95\n)\n</code></pre>"},{"location":"core-concepts/classification/tree/#level-based-contracts","title":"Level-Based Contracts","text":"<p>When implementing tree-based classification, consider contract complexity at each level:</p> <ul> <li> <p>First Level: Use minimal fields for broad categorization   <pre><code>class FinancialContract(Contract):\n    total_amount: int  # Just key identifying fields\n</code></pre></p> </li> <li> <p>Second Level: Include full field set for precise classification   <pre><code>class InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    lines: List[LineItem]  # Complete field set\n    total_amount: float\n</code></pre></p> </li> </ul>"},{"location":"core-concepts/classification/vision/","title":"Vision Classification","text":"<p>A document is not only text but also structure, color, and other numerous features that disappear when OCR is used. Vision classification leverages these visual elements to improve accuracy, particularly important for specific document types.</p>"},{"location":"core-concepts/classification/vision/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import Process, Classification\nfrom extract_thinker.document_loader import DocumentLoaderTesseract\n\n# Define classifications with example images\nclassifications = [\n    Classification(\n        name=\"Driver License\",\n        description=\"This is a driver license\",\n        contract=DriverLicense,\n        image=\"path/to/example_license.png\"  # Example image helps model understand\n    ),\n    Classification(\n        name=\"Invoice\",\n        description=\"This is an invoice\",\n        contract=InvoiceContract,\n        image=\"path/to/example_invoice.png\"\n    )\n]\n\n# Initialize process with vision-capable model\nprocess = Process()\nprocess.add_classify_extractor([[\n    Extractor(DocumentLoaderTesseract(tesseract_path))\n    .load_llm(\"gpt-4o\")  # Vision-capable model\n]])\n\n# Classify with vision enabled\nresult = process.classify(\n    \"document.pdf\",\n    classifications,\n    image=True  # Enable vision processing\n)\n</code></pre>"},{"location":"core-concepts/classification/vision/#benefits-and-tradeoffs","title":"Benefits and Tradeoffs","text":""},{"location":"core-concepts/classification/vision/#benefits","title":"Benefits","text":"<ul> <li>Better handling of document layouts</li> <li>Recognition of visual patterns and structures</li> <li>Improved accuracy for visually distinct documents</li> <li>Ability to understand non-textual elements</li> </ul>"},{"location":"core-concepts/classification/vision/#tradeoffs","title":"Tradeoffs","text":"<ul> <li>Higher cost due to image processing</li> <li>Larger context window requirements</li> <li>Longer processing times</li> <li>Higher token usage</li> </ul>"},{"location":"core-concepts/classification/vision/#model-selection","title":"Model Selection","text":"<p>Different models offer varying capabilities for vision tasks:</p> <ul> <li>GPT-4 Vision: Supports low/high/auto quality settings (85 tokens for low)</li> <li>Claude 3 Sonnet: Full vision capabilities without quality options</li> <li>Azure Phi-3 Vision: Cost-effective alternative</li> </ul>"},{"location":"core-concepts/classification/vision/#best-practices","title":"Best Practices","text":"<ul> <li>Use compressed images when possible to reduce costs</li> <li>Provide high-quality example images for each classification</li> <li>Consider using a mix of vision and text-based classification</li> <li>Use appropriate image quality settings based on needs</li> <li>Cache vision results to avoid reprocessing</li> </ul>"},{"location":"core-concepts/classification/vision/#example-with-multiple-models","title":"Example with Multiple Models","text":"<pre><code># Initialize extractors with different vision models\ngpt4_vision = Extractor(document_loader)\ngpt4_vision.load_llm(\"gpt-4-vision\")\n\nclaude_vision = Extractor(document_loader)\nclaude_vision.load_llm(\"claude-3-sonnet\")\n\nphi3_vision = Extractor(document_loader)\nphi3_vision.load_llm(\"phi-3-vision\")\n\n# Create process with vision models\nprocess = Process()\nprocess.add_classify_extractor([\n    [phi3_vision],           # Cost-effective first attempt\n    [claude_vision, gpt4_vision]  # More capable models if needed\n])\n\n# Classify with vision and consensus\nresult = process.classify(\n    \"document.pdf\",\n    classifications,\n    strategy=ClassificationStrategy.CONSENSUS_WITH_THRESHOLD,\n    threshold=9,\n    image=True\n)\n</code></pre>"},{"location":"core-concepts/completion-strategies/","title":"Completion Strategies","text":"<p>ExtractThinker provides different strategies for handling document content processing through LLMs, especially when dealing with content that might exceed the model's context window. There are three main strategies: Forbidden, Concatenate, and Paginate.</p>"},{"location":"core-concepts/completion-strategies/#forbidden-strategy","title":"FORBIDDEN Strategy","text":"<p>The FORBIDDEN strategy is the default approach - it prevents processing of content that exceeds the model's context window. This is the simplest strategy, while larger content can be handled using other available strategies.</p> <pre><code>from extract_thinker import Extractor\nfrom extract_thinker.models.completion_strategy import CompletionStrategy\n\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\n# Will raise ValueError if content is too large\nresult = extractor.extract(\n    file_path,\n    ResponseModel,\n    completion_strategy=CompletionStrategy.FORBIDDEN # Default\n)\n</code></pre> <p>For more advanced strategies that handle larger content, see:</p> <ul> <li>CONCATENATE Strategy - For handling content larger than the context window</li> <li>PAGINATE Strategy - For processing multi-page documents in parallel</li> </ul> <p>The choice of completion strategy depends on your specific use case:</p> <p>Use FORBIDDEN when:</p> <ul> <li>Content is guaranteed to fit in context window</li> <li>You need the simplest possible processing and default behavior</li> <li>You want to ensure content is processed as a single unit</li> </ul> <p>Use CONCATENATE when:</p> <ul> <li>Content might exceed context window</li> <li>The size exceeds the output but not the input context window.</li> <li>You want automatic handling of large content</li> </ul> <p>Use PAGINATE when:</p> <ul> <li>Processing multi-page documents</li> <li>The size exceeds the output but and the input context window.</li> <li>You need sophisticated conflict resolution between pages</li> </ul>"},{"location":"core-concepts/completion-strategies/concatenate/","title":"Concatenate Strategy","text":"<p>The Concatenate strategy is designed to handle content that exceeds the LLM's context window by splitting it into manageable chunks, processing them separately, and then combining the results.</p>"},{"location":"core-concepts/completion-strategies/concatenate/#how-it-works","title":"How It Works","text":"<p>1. Initial Request</p> <ul> <li>Sends the content to the LLM with the desired response structure</li> <li>Monitors the LLM's response completion status</li> </ul> <p>2. Continuation Process</p> <ul> <li>If response is truncated (finish_reason=\"length\"), builds a continuation request</li> <li>Includes previous partial response for context</li> <li>Continues until LLM indicates completion</li> </ul> <p>3. Validation</p> <ul> <li>When LLM indicates completion (finish_reason=\"stop\")</li> <li>Validates the combined JSON response</li> <li>Raises error if invalid JSON is received on completion</li> </ul> <p>4. Response Processing</p> <ul> <li>Combines all response parts</li> <li>Validates against the specified response model</li> <li>Returns structured data</li> </ul>"},{"location":"core-concepts/completion-strategies/concatenate/#usage","title":"Usage","text":"<pre><code>from extract_thinker import Extractor\nfrom extract_thinker.models.completion_strategy import CompletionStrategy\n\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\nresult = extractor.extract(\n    file_path,\n    ResponseModel,\n    completion_strategy=CompletionStrategy.CONCATENATE\n)\n</code></pre>"},{"location":"core-concepts/completion-strategies/concatenate/#benefits","title":"Benefits","text":"<ul> <li>Handles Large Content: Can process documents larger than the output context window</li> <li>Maintains Context: Attempts to keep related content together</li> </ul>"},{"location":"core-concepts/completion-strategies/concatenate/#implementation-details","title":"Implementation Details","text":"Concatenation Handler Implementation <p>The ConcatenationHandler implements the CONCATENATE strategy: <pre><code>import copy\nimport yaml\nimport json\nfrom typing import Any, Dict, List, Optional\nfrom pydantic import BaseModel\nfrom extract_thinker.completion_handler import CompletionHandler\nfrom extract_thinker.utils import encode_image, add_classification_structure\n\nclass ConcatenationHandler(CompletionHandler):\n    def __init__(self, llm):\n        super().__init__(llm)\n        self.json_parts = []\n\n    def _is_valid_json_continuation(self, response: str) -&gt; bool:\n        \"\"\"Check if the response is a valid JSON continuation.\"\"\"\n        cleaned_response = response.strip()\n\n        # Check if response contains JSON markers\n        has_json_markers = (\n            \"```json\" in cleaned_response or \n            \"{\" in cleaned_response or \n            \"[\" in cleaned_response\n        )\n\n        return has_json_markers\n\n    def handle(self, content: Any, response_model: type[BaseModel], vision: bool = False, extra_content: Optional[str] = None) -&gt; Any:\n        self.json_parts = []\n        messages = self._build_messages(content, vision, response_model)\n\n        if extra_content:\n            self._add_extra_content(messages, extra_content)\n\n        retry_count = 0\n        max_retries = 3\n        while True:\n            try:\n                response = self.llm.raw_completion(messages)\n\n                # Validate if it's a proper JSON continuation\n                if not self._is_valid_json_continuation(response):\n                    retry_count += 1\n                    if retry_count &gt;= max_retries:\n                        raise ValueError(\"Maximum retries reached with invalid JSON continuations\")\n                    continue\n\n                self.json_parts.append(response)\n\n                # Try to process and validate the JSON\n                result = self._process_json_parts(response_model)\n                return result\n\n            except ValueError as e:\n                if retry_count &gt;= max_retries:\n                    raise ValueError(f\"Maximum retries reached: {str(e)}\")\n                retry_count += 1\n                messages = self._build_continuation_messages(messages, response)\n\n    def _process_json_parts(self, response_model: type[BaseModel]) -&gt; Any:\n        \"\"\"Process collected JSON parts into a complete response.\"\"\"\n        if not self.json_parts:\n            raise ValueError(\"No JSON content collected\")\n\n        processed_parts = []\n        for content in self.json_parts:\n            # Remove code fences and extraneous formatting artifacts\n            cleaned = (content\n                       .replace('```json', '')\n                       .replace('```', '')\n                       .replace('\\njson', '')\n                       .replace('\\n', ' ')\n                       .strip())\n\n            # If there's still something left after cleaning, keep it\n            if cleaned:\n                processed_parts.append(cleaned)\n\n        if not processed_parts:\n            raise ValueError(\"No valid JSON content found in the response\")\n\n        # Combine all cleaned parts into one string\n        combined_json = \"\".join(processed_parts)\n\n        # Attempt to parse the combined JSON\n        try:\n            parsed = json.loads(combined_json)\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Failed to parse combined JSON: {str(e)}\\nJSON: {combined_json}\")\n\n        # Validate the parsed JSON against the Pydantic model\n        try:\n            return response_model.model_validate(parsed)\n        except Exception as e:\n            raise ValueError(f\"Failed to validate parsed JSON: {str(e)}\\nJSON: {combined_json}\")\n\n    def _build_continuation_messages(\n        self,\n        messages: List[Dict[str, Any]],\n        partial_content: str\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build messages for continuation request.\"\"\"\n        continuation_messages = copy.deepcopy(messages)\n\n        # Add partial response as assistant message\n        continuation_messages.append({\n            \"role\": \"assistant\",\n            \"content\": partial_content\n        })\n\n        # Add continuation prompt\n        continuation_messages.append({\n            \"role\": \"user\", \n            \"content\": \"## CONTINUE JSON\"\n        })\n\n        return continuation_messages\n\n    def _build_messages(self, content: Any, vision: bool, response_model: type[BaseModel]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build messages for LLM request.\"\"\"\n        system_message = {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are a server API that receives document information and returns specific fields in JSON format.\\n\"\n                \"Please follow the response structure exactly as specified below.\\n\\n\"\n                f\"{add_classification_structure(response_model)}\\n\"\n            )\n        }\n\n        if vision:\n            message_content = self._build_vision_content(content)\n            messages = [\n                system_message,\n                {\n                    \"role\": \"user\",\n                    \"content\": message_content\n                }\n            ]\n        else:\n            message_content = self._build_text_content(content)\n            messages = [\n                system_message,\n                {\n                    \"role\": \"user\",\n                    \"content\": message_content\n                }\n            ]\n\n        return messages\n\n    def _build_vision_content(self, content: Any) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build content for vision request.\"\"\"\n        message_content = []\n\n        if isinstance(content, list):\n            # Handle list of content items\n            for item in content:\n                # Add text content if available\n                if isinstance(item, dict) and \"content\" in item:\n                    message_content.append({\n                        \"type\": \"text\",\n                        \"text\": f\"##Content\\n\\n{item['content']}\"\n                    })\n\n                # Add images if available\n                if isinstance(item, dict):\n                    images = []\n                    if \"images\" in item and isinstance(item[\"images\"], list):\n                        images.extend(item[\"images\"])\n                    if \"image\" in item and item[\"image\"] is not None:\n                        images.append(item[\"image\"])\n\n                    for img in images:\n                        if img:\n                            message_content.append({\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\"\n                                }\n                            })\n        else:\n            # Handle single item\n            if isinstance(content, dict):\n                # Add text content if available\n                if \"content\" in content:\n                    message_content.append({\n                        \"type\": \"text\",\n                        \"text\": f\"##Content\\n\\n{content['content']}\"\n                    })\n\n                # Add images\n                images = []\n                if \"images\" in content and isinstance(content[\"images\"], list):\n                    images.extend(content[\"images\"])\n                if \"image\" in content and content[\"image\"] is not None:\n                    images.append(content[\"image\"])\n\n                for img in images:\n                    if img:\n                        message_content.append({\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\"\n                            }\n                        })\n\n        return message_content\n\n    def _build_text_content(self, content: Any) -&gt; str:\n        \"\"\"Build content for text request.\"\"\"\n        if isinstance(content, dict):\n            return f\"##Content\\n\\n{yaml.dump(content)}\"\n        elif isinstance(content, str):\n            return f\"##Content\\n\\n{content}\"\n        else:\n            return f\"##Content\\n\\n{str(content)}\"\n\n    def _add_extra_content(self, messages: List[Dict[str, Any]], extra_content: str) -&gt; None:\n        \"\"\"Add extra content to messages.\"\"\"\n        messages.insert(1, {\n            \"role\": \"user\",\n            \"content\": f\"##Extra Content\\n\\n{extra_content}\"\n        })\n</code></pre></p>"},{"location":"core-concepts/completion-strategies/concatenate/#when-to-use","title":"When to Use","text":"<p>CONCATENATE is the best choice when:</p> <p>Context window is large</p> <ul> <li>For models like gpt-4o, claude-3-5-sonnet, etc.</li> </ul> <p>The content is not too large</p> <ul> <li>Should be used for documents that are not too large (e.g. 500 pages)</li> </ul> <p>For handling bigger documents, consider using the PAGINATE strategy.</p>"},{"location":"core-concepts/completion-strategies/paginate/","title":"PAGINATE Strategy","text":"<p>The PAGINATE strategy processes multi-page documents by handling each page independently and then intelligently merging the results, including sophisticated conflict resolution when pages contain overlapping information.</p>"},{"location":"core-concepts/completion-strategies/paginate/#how-it-works","title":"How It Works","text":"<p>Page Separation</p> <ul> <li>Identifies individual pages</li> <li>Preserves page metadata</li> <li>Maintains document structure</li> </ul> <p>Parallel Processing</p> <ul> <li>Each page processed independently</li> <li>Uses full context window per page</li> <li>Handles page-specific content</li> </ul> <p>Result Collection</p> <ul> <li>Gathers results from all pages</li> <li>Validates individual page results</li> <li>Prepares for merging</li> </ul> <p>Conflict Resolution</p> <ul> <li>Detects overlapping information</li> <li>Resolves conflicts using confidence scores</li> <li>Maintains data consistency</li> </ul>"},{"location":"core-concepts/completion-strategies/paginate/#usage","title":"Usage","text":"<pre><code>from extract_thinker import Extractor\nfrom extract_thinker.models.completion_strategy import CompletionStrategy\n\nextractor = Extractor()\nextractor.load_llm(\"gpt-4\")\n\nresult = extractor.extract(\n    file_path,\n    ResponseModel,\n    completion_strategy=CompletionStrategy.PAGINATE\n)\n</code></pre>"},{"location":"core-concepts/completion-strategies/paginate/#benefits","title":"Benefits","text":"<ul> <li>Cheaper: Reduced parallel context window would be cheaper than a long Concatenate Strategy</li> <li>Parallel Processing: Pages can be processed independently</li> <li>Conflict Resolution: Smart merging of results from different pages</li> <li>Scalability: Handles documents of any length</li> <li>Accuracy: Each page gets full context window attention</li> </ul>"},{"location":"core-concepts/completion-strategies/paginate/#implementation-details","title":"Implementation Details","text":"Pagination Handler Implementation <p>The PaginationHandler implements the PAGINATE strategy: <pre><code>import copy\nfrom typing import Any, Dict, List, Optional, get_origin, get_args, Union\nfrom pydantic import BaseModel, Field\nfrom instructor.exceptions import IncompleteOutputException\nfrom extract_thinker.completion_handler import CompletionHandler\nfrom extract_thinker.utils import encode_image, json_to_formatted_string, make_all_fields_optional\nimport yaml\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nclass ConflictResolution(BaseModel):\n    resolved_fields: Dict[str, Dict[str, Any]] = Field(\n        description=\"Dictionary of resolved field values with confidence scores\",\n        default_factory=dict\n    )\n\nclass PaginationHandler(CompletionHandler):\n    def __init__(self, llm):\n        super().__init__(llm)\n\n    def _make_hashable(self, item: Any) -&gt; Any:\n        \"\"\"Recursively convert a value to something hashable.\"\"\"\n        if isinstance(item, dict):\n            return tuple(sorted((k, self._make_hashable(v)) for k, v in item.items()))\n        elif isinstance(item, list):\n            return tuple(self._make_hashable(x) for x in item)\n        return item\n\n    def handle(self, \n               content: List[Dict[str, Any]],\n               response_model: type[BaseModel],\n               vision: bool = False,\n               extra_content: Optional[str] = None) -&gt; Any:\n        # Make fields optional to allow partial results\n        response_model_optional = make_all_fields_optional(response_model)\n\n        # Process pages in parallel\n        results = []\n        with ThreadPoolExecutor() as executor:\n            futures = []\n            for page in content:\n                # Build messages for each page\n                messages = self._build_messages(page, vision)\n                if extra_content:\n                    self._add_extra_content(messages, extra_content)\n\n                # Submit page processing task\n                future = executor.submit(\n                    self._process_page, \n                    messages,\n                    response_model_optional\n                )\n                futures.append(future)\n\n            # Collect results as they complete\n            for future in as_completed(futures):\n                try:\n                    result = future.result()\n                    results.append(result)\n                except Exception as e:\n                    # Log error but continue processing other pages\n                    print(f\"Error processing page: {str(e)}\")\n\n        if not results:\n            raise ValueError(\"No valid results obtained from any page\")\n\n        # Pair up the pages with their results for context\n        pages_data = list(zip(content, results))\n\n        return self._merge_results(results, response_model, pages_data)\n\n    def _process_page(self, messages: List[Dict[str, Any]], response_model: type[BaseModel]) -&gt; Any:\n        \"\"\"Process a single page with retry logic for incomplete responses\"\"\"\n        try:\n            return self.llm.request(messages, response_model)\n        except IncompleteOutputException as e:\n            # Handle partial response\n            partial_result = self._handle_partial_response(e, messages, response_model)\n            partial_dict = partial_result.model_dump()\n            if self._has_conflicts(partial_dict, response_model):\n                # We'll resolve conflicts later after merging all pages\n                return partial_result\n            return partial_result\n\n    def _merge_results(self, results: List[Any], response_model: type[BaseModel], pages_data: List[Any]) -&gt; Any:\n        \"\"\"Merge results from multiple pages into a dictionary, detect conflicts, resolve if needed, then return model.\"\"\"\n\n        # Collect all values for each field\n        field_values = {}\n        for _, result in pages_data:\n            result_dict = result.model_dump()\n            for field_name, field_value in result_dict.items():\n                field_values.setdefault(field_name, []).append(field_value)\n\n        # Merge fields\n        merged = {}\n        for field_name, values in field_values.items():\n            # Get the annotated type from the response model\n            field_type = response_model.model_fields[field_name].annotation if field_name in response_model.model_fields else None\n            non_null_values = [v for v in values if v is not None]\n\n            if field_type and get_origin(field_type) is list:\n                # Merge list fields using a more sophisticated approach\n                merged_list = self._merge_list_field(field_name, values, field_type)\n                merged[field_name] = merged_list\n            else:\n                # Scalar field handling\n                if len(non_null_values) == 0:\n                    # If the field is expected to be a string, default to an empty string.\n                    if field_type == str or (get_origin(field_type) is Union and str in get_args(field_type)):\n                        merged[field_name] = \"\"\n                    else:\n                        continue\n                else:\n                    # Build a mapping from the hashable version of each candidate to the original candidate.\n                    distinct_map = {}\n                    for candidate in non_null_values:\n                        key = self._make_hashable(candidate)\n                        if key not in distinct_map:\n                            distinct_map[key] = candidate\n                    distinct_values = list(distinct_map.values())\n\n                    if len(distinct_values) == 1:\n                        merged[field_name] = distinct_values[0]\n                    else:\n                        # Store conflicts in a special structure\n                        merged[field_name] = {\n                            \"_conflict\": True,\n                            \"candidates\": distinct_values\n                        }\n\n        # Check for conflicts and resolve if necessary\n        if self._has_conflicts(merged, response_model):\n            merged = self._resolve_conflicts(merged, response_model, pages_data, field_values)\n\n        # Clean merged dictionary to ensure it's compatible with the response model\n        merged = self._clean_merged_dict(merged, response_model)\n\n        # Filter out any keys with a None value,\n        # now every required field (e.g., a string like \"thinking\") will be non-null.\n        merged = {k: v for k, v in merged.items() if v is not None}\n\n        return response_model(**merged)\n\n    def _merge_list_field(self, field_name: str, values: List[Any], field_type: Any) -&gt; List[Any]:\n        \"\"\"\n        Merge list fields from multiple pages. If the list is a list of Pydantic models,\n        we try to merge based on a unique key field (e.g. `country` for countries, `region` for regions).\n        If it's not a list of models or no unique key is found, we just concatenate and\n        rely on conflict resolution later.\n        \"\"\"\n        # Flatten all lists\n        flattened = []\n        for v in values:\n            if isinstance(v, list):\n                flattened.extend(v)\n            elif v is not None:\n                flattened.append(v)\n\n        # Attempt to detect if we're dealing with a Pydantic model list\n        args = get_args(field_type)\n        if args:\n            model_type = args[0]\n            if hasattr(model_type, '__fields__'):\n                # We have a Pydantic model class in the list\n                # Identify a unique key to merge on. \n                candidate_keys = ['country', 'region', 'id', 'name']\n                unique_key = None\n                for ck in candidate_keys:\n                    if ck in model_type.__fields__:\n                        unique_key = ck\n                        break\n\n                if unique_key:\n                    # Merge by unique key using case-insensitive comparison\n                    merged_by_key = {}\n                    for item in flattened:\n                        if hasattr(item, 'model_dump'):\n                            item_dict = item.model_dump()\n                        else:\n                            item_dict = item\n                        key_val = item_dict.get(unique_key)\n                        if key_val is not None:\n                            normalized_key = str(key_val).lower()\n                            if normalized_key in merged_by_key:\n                                merged_by_key[normalized_key] = self._merge_two_models(\n                                    merged_by_key[normalized_key],\n                                    item_dict\n                                )\n                            else:\n                                merged_by_key[normalized_key] = item_dict\n                        else:\n                            # If no unique key found for this item, just store it uniquely\n                            merged_by_key[f\"no_key_{len(merged_by_key)}\"] = item_dict\n\n                    return list(merged_by_key.values())\n                else:\n                    # No unique key found, just return flattened list\n                    return flattened\n            else:\n                # Not a pydantic model list\n                return flattened\n        else:\n            # Not a parametrized list type\n            return flattened\n\n    def _merge_two_models(self, existing: Dict[str, Any], new: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Merge two dictionaries representing the same entity. For scalar values,\n        prefer the existing if both are non-null or prefer non-null values.\n        For lists, combine them.\n        \"\"\"\n        merged = existing.copy()\n        for k, v in new.items():\n            if k not in merged or merged[k] is None:\n                merged[k] = v\n            else:\n                if isinstance(merged[k], list) and isinstance(v, list):\n                    # Extend lists\n                    merged[k].extend(v)\n                # If there's a scalar conflict, we can keep the first non-null,\n                # or implement custom conflict handling here.\n                # For now, do nothing if both have a value, keep existing.\n        return merged\n\n    def _clean_merged_dict(self, merged: Dict[str, Any], response_model: type[BaseModel]) -&gt; Dict[str, Any]:\n        \"\"\"Clean the merged dictionary after conflict resolution to remove any leftover special structures \n        and ensure values are compatible with the response model.\"\"\"\n        cleaned = {}\n        for field_name, field_value in merged.items():\n            # If there's still a conflict structure, remove it or handle it\n            if isinstance(field_value, dict) and field_value.get(\"_conflict\"):\n                # If somehow unresolved (shouldn't happen), just pick one candidate or None\n                candidates = field_value.get(\"candidates\", [])\n                cleaned[field_name] = candidates[0] if candidates else None\n            else:\n                cleaned[field_name] = field_value\n\n        # Pydantic will do additional type coercion upon instantiation\n        return cleaned\n\n    def _has_conflicts(self, result_dict: Dict[str, Any], response_model: type[BaseModel]) -&gt; bool:\n        \"\"\"Check if result dictionary has any conflicting fields.\"\"\"\n        for field_name, field_value in result_dict.items():\n            if field_name not in response_model.model_fields:\n                continue\n            field_type = response_model.model_fields[field_name].annotation\n\n            # Check for special conflict dictionary (scalar conflict)\n            if isinstance(field_value, dict) and field_value.get(\"_conflict\"):\n                return True\n\n            # Check list field duplicates (if needed)\n            # Here you could implement more robust checks if required.\n        return False\n\n    def _identify_conflicts(self, result_dict: Dict[str, Any], response_model: type[BaseModel]) -&gt; Dict[str, Any]:\n        \"\"\"Identify conflicting fields in the result dictionary.\"\"\"\n        conflicts = {}\n        for field_name, field_value in result_dict.items():\n            if field_name not in response_model.model_fields:\n                continue\n            field_type = response_model.model_fields[field_name].annotation\n\n            # Scalar conflict\n            if isinstance(field_value, dict) and field_value.get(\"_conflict\"):\n                conflicts[field_name] = field_value[\"candidates\"]\n            # Could add checks for list conflicts if needed.\n        return conflicts\n\n    def _resolve_conflicts(self, result_dict: Dict[str, Any], response_model: type[BaseModel],\n                           pages_data: List[Any], field_values: Dict[str, List[Any]]) -&gt; Dict[str, Any]:\n        \"\"\"Resolve conflicts in the dictionary using the LLM.\"\"\"\n        conflicts = self._identify_conflicts(result_dict, response_model)\n\n        if not conflicts:\n            return result_dict\n\n        resolved = self._request_conflict_resolution(conflicts, pages_data, field_values)\n        return self._merge_resolved_conflicts(result_dict, resolved, response_model)\n\n    def _request_conflict_resolution(\n        self,\n        conflicts: Dict[str, List[Any]],\n        pages_data: List[Any],\n        field_values: Dict[str, List[Any]]\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Request LLM to resolve conflicts.\"\"\"\n        message_content = self._build_conflict_resolution_prompt(conflicts, pages_data, field_values)\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a server API that resolves field conflicts. You have access to the original page contents and the conflicting values extracted from them.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": message_content\n            }\n        ]\n\n        try:\n            response: ConflictResolution = self.llm.request(messages, ConflictResolution)\n            return response.resolved_fields\n        except Exception as e:\n            raise ValueError(f\"Failed to resolve conflicts: {str(e)}. This may happen if the context was too big or the LLM couldn't resolve the conflicts.\")\n\n    def _build_conflict_resolution_prompt(\n        self,\n        conflicts: Dict[str, List[Any]],\n        pages_data: List[Any],\n        field_values: Dict[str, List[Any]]\n    ) -&gt; Union[str, List[Dict[str, Any]]]:\n        \"\"\"Build prompt for conflict resolution with context from all pages.\"\"\"\n        # Check if any page has vision content\n        has_vision_content = any(\n            isinstance(page_content, dict) and ('image' in page_content or 'images' in page_content)\n            for page_content, _ in pages_data\n        )\n\n        if has_vision_content:\n            # Build vision-compatible message content\n            message_content = []\n\n            # Add initial text content\n            intro_text = [\n                \"Please resolve conflicts in these fields by choosing the correct value and providing a confidence score (1-10).\",\n                \"You have the contents of each page that contributed data, and the conflicting values they produced.\",\n                \"Return JSON in this format:\\n{\\n  \\\"resolved_fields\\\": {\\n    \\\"field_name\\\": {\\\"value\\\": \\\"chosen_value\\\", \\\"confidence\\\": 9}\\n  }\\n}\\n\",\n                \"Conflicts to resolve:\",\n                str(conflicts),\n                \"\\nHere are the original pages and their extracted values:\"\n            ]\n\n            message_content.append({\n                \"type\": \"text\",\n                \"text\": \"\\n\".join(intro_text)\n            })\n\n            # Add each page's content and images\n            for i, (page_content, page_result) in enumerate(pages_data):\n                page_text = [f\"\\n--- Page {i+1} ---\", \"Original page content:\"]\n\n                if isinstance(page_content, dict):\n                    # Add text content if available\n                    content_data = self._process_content_data(page_content)\n                    if content_data:\n                        page_text.append(content_data)\n\n                    # Add extracted values\n                    page_text.append(\"Extracted values for all fields on this page:\")\n                    for field_name, values_for_field in field_values.items():\n                        page_value = values_for_field[i] if i &lt; len(values_for_field) else None\n                        page_text.append(f\"{field_name}: {page_value}\")\n\n                    message_content.append({\n                        \"type\": \"text\",\n                        \"text\": \"\\n\".join(page_text)\n                    })\n\n                    # Add images if present\n                    if 'image' in page_content:\n                        message_content.append({\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{encode_image(page_content['image'])}\"\n                            }\n                        })\n                    elif 'images' in page_content:\n                        for img in page_content['images']:\n                            message_content.append({\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\"\n                                }\n                            })\n\n            return message_content\n        else:\n            # Build regular text prompt\n            prompt_parts = []\n            prompt_parts.extend([\n                \"Please resolve conflicts in these fields by choosing the correct value and providing a confidence score (1-10).\",\n                \"You have the contents of each page that contributed data, and the conflicting values they produced.\",\n                \"Return JSON in this format:\\n{\\n  \\\"resolved_fields\\\": {\\n    \\\"field_name\\\": {\\\"value\\\": \\\"chosen_value\\\", \\\"confidence\\\": 9}\\n  }\\n}\\n\",\n                \"Conflicts to resolve:\",\n                str(conflicts),\n                \"\\nHere are the original pages and their extracted values:\"\n            ])\n\n            for i, (page_content, page_result) in enumerate(pages_data):\n                prompt_parts.extend([\n                    f\"\\n--- Page {i+1} ---\",\n                    \"Original page content:\",\n                    yaml.dump(page_content) if isinstance(page_content, dict) else str(page_content),\n                    \"Extracted values for all fields on this page:\"\n                ])\n\n                for field_name, values_for_field in field_values.items():\n                    page_value = values_for_field[i] if i &lt; len(values_for_field) else None\n                    prompt_parts.append(f\"{field_name}: {page_value}\")\n\n            return \"\\n\".join(prompt_parts)\n\n    def _process_content_data(self, content: Union[Dict[str, Any], List[Any], str]) -&gt; Optional[str]:\n        \"\"\"Process content data by filtering out images and converting to a string.\"\"\"\n        if isinstance(content, dict):\n            filtered_content = {\n                k: v for k, v in content.items()\n                if k not in ('images', 'image') and not hasattr(v, 'read')\n            }\n            if filtered_content.get(\"is_spreadsheet\", False):\n                content_str = json_to_formatted_string(filtered_content.get(\"data\", {}))\n            else:\n                content_str = yaml.dump(filtered_content, default_flow_style=True)\n            return content_str\n        return None\n\n    def _merge_resolved_conflicts(\n        self,\n        original: Dict[str, Any],\n        resolved: Dict[str, Dict[str, Any]],\n        response_model: type[BaseModel]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Merge resolved conflicts back into the dictionary.\"\"\"\n        result_dict = copy.deepcopy(original)\n\n        for field_name, resolution in resolved.items():\n            if field_name in result_dict:\n                result_dict[field_name] = resolution[\"value\"]\n\n        return result_dict\n\n    def _handle_partial_response(\n        self,\n        exception: IncompleteOutputException,\n        messages: List[Dict[str, Any]],\n        response_model: type[BaseModel]\n    ) -&gt; Any:\n        \"\"\"Handle partial response by continuing the request\"\"\"\n        partial_content = exception.last_completion.choices[0].message.content\n        continuation_messages = self._build_continuation_messages(messages, partial_content)\n\n        try:\n            return self.llm.request(continuation_messages, response_model)\n        except Exception as e:\n            raise ValueError(f\"Failed to complete partial response: {str(e)}\")\n\n    def _build_continuation_messages(\n        self,\n        messages: List[Dict[str, Any]],\n        partial_content: str\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build messages for continuation request.\"\"\"\n        continuation_messages = copy.deepcopy(messages)\n\n        # Add partial response as assistant message\n        continuation_messages.append({\n            \"role\": \"assistant\",\n            \"content\": partial_content\n        })\n\n        # Add continuation prompt\n        continuation_messages.append({\n            \"role\": \"user\", \n            \"content\": \"## CONTINUE JSON\"\n        })\n\n        return continuation_messages\n\n    def _build_messages(self, content: Any, vision: bool) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build messages for LLM request.\"\"\"\n        system_message = {\n            \"role\": \"system\",\n            \"content\": \"You are a server API that receives document information and returns specific fields in JSON format.\"\n        }\n\n        if vision:\n            message_content = self._build_vision_content(content)\n            messages = [\n                system_message,\n                {\n                    \"role\": \"user\",\n                    \"content\": message_content\n                }\n            ]\n        else:\n            message_content = self._build_text_content(content)\n            messages = [\n                system_message,\n                {\n                    \"role\": \"user\",\n                    \"content\": message_content\n                }\n            ]\n\n        return messages\n\n    def _build_vision_content(self, content: Any) -&gt; List[Dict[str, Any]]:\n        \"\"\"Build content for vision request.\"\"\"\n        message_content = []\n\n        # If there's textual 'content', push it first\n        if isinstance(content, dict) and \"content\" in content:\n            message_content.append({\n                \"type\": \"text\",\n                \"text\": f\"##Content\\n\\n{content['content']}\"\n            })\n\n        # Now handle multiple images\n        if isinstance(content, dict):\n            images = []\n            if \"images\" in content and isinstance(content[\"images\"], list):\n                images.extend(content[\"images\"])\n            if \"image\" in content and content[\"image\"] is not None:\n                images.append(content[\"image\"])\n\n            for img in images:\n                if img:\n                    message_content.append({\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\"\n                        }\n                    })\n        return message_content\n\n    def _build_text_content(self, content: Any) -&gt; str:\n        \"\"\"Build content for text request.\"\"\"\n        if isinstance(content, dict):\n            return f\"##Content\\n\\n{yaml.dump(content)}\"\n        elif isinstance(content, str):\n            return f\"##Content\\n\\n{content}\"\n        else:\n            return f\"##Content\\n\\n{str(content)}\"\n\n    def _add_extra_content(self, messages: List[Dict[str, Any]], extra_content: str) -&gt; None:\n        \"\"\"Add extra content to messages.\"\"\"\n        messages.insert(1, {\n            \"role\": \"user\",\n            \"content\": f\"##Extra Content\\n\\n{extra_content}\"\n        })\n</code></pre></p>"},{"location":"core-concepts/completion-strategies/paginate/#when-to-use","title":"When to Use","text":"<p>PAGINATE is the best choice when:</p> <p>Context window is small</p> <ul> <li>For local LLMs with smaller context windows (e.g Llama 3.3 8k context window).</li> </ul> <p>The content is too Big</p> <ul> <li>When the file will not fit in the entire context window (e.g 500 page document)</li> </ul> <p>Model Accuracy</p> <ul> <li>Sometimes LLMs can lose focus when the context is too big, Paginate strategy will solve this problem</li> </ul>"},{"location":"core-concepts/contracts/","title":"Contracts","text":"<p>\ud83d\udea7 In Development</p> <p>This component is currently under active development. The API might change in future releases.</p> <p>Contracts in ExtractThinker are Pydantic models that define the structure of data you want to extract. They provide type safety and validation for your extracted data.</p> Base Contract Implementation <pre><code>from pydantic import BaseModel\n\n\nclass Contract(BaseModel):\n    pass\n</code></pre>"},{"location":"core-concepts/contracts/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import Contract\nfrom typing import List, Optional\nfrom pydantic import Field\n\nclass InvoiceLineItem(Contract):\n    description: str = Field(description=\"Description of the item\")\n    quantity: int = Field(description=\"Quantity of items\")\n    unit_price: float = Field(description=\"Price per unit\")\n    amount: float = Field(description=\"Total amount for line\")\n\nclass InvoiceContract(Contract):\n    invoice_number: str = Field(description=\"Invoice identifier\")\n    date: str = Field(description=\"Invoice date\")\n    total_amount: float = Field(description=\"Total invoice amount\")\n    line_items: List[InvoiceLineItem] = Field(description=\"List of items in invoice\")\n    notes: Optional[str] = Field(description=\"Additional notes\", default=None)\n</code></pre>"},{"location":"core-concepts/document-loaders/","title":"Document Loaders","text":"<p>Document Loaders are the foundation of ExtractThinker's document processing pipeline. They handle the initial loading and preprocessing of documents, converting them into a standardized format that can be used by other components.</p>"},{"location":"core-concepts/document-loaders/#basic-concept","title":"Basic Concept","text":"<p>A Document Loader can return content in two formats: - A simple string containing the extracted text - A structured object with pages and their content, that depends on the loader</p> Base Document Loader <p>The base DocumentLoader class defines the interface and common functionality that all loaders must implement: - <code>load_content_from_file</code>: Process files from disk - <code>load_content_from_stream</code>: Process BytesIO streams - <code>can_handle</code>: Validate file types - <code>convert_to_images</code>: Convert documents to images <pre><code>from abc import ABC, abstractmethod\nimport io\nfrom io import BytesIO\nfrom PIL import Image\nimport pypdfium2 as pdfium\nfrom typing import Any, Dict, Union, List\nfrom cachetools import TTLCache\nimport os\nimport magic\nfrom extract_thinker.utils import get_file_extension, check_mime_type\nfrom playwright.sync_api import sync_playwright\nfrom urllib.parse import urlparse\nimport base64\nimport math\n\nclass DocumentLoader(ABC):\n    # SUPPORTED_FORMATS = [\n    #     \"pdf\", \"jpg\", \"jpeg\", \"png\", \"tiff\", \"bmp\"\n    # ]\n\n    def __init__(self, content: Any = None, cache_ttl: int = 300, screenshot_timeout: int = 1000):\n        \"\"\"Initialize loader.\n\n        Args:\n            content: Initial content\n            cache_ttl: Cache time-to-live in seconds\n            screenshot_timeout: Timeout in milliseconds to wait for page content load when capturing a screenshot.\n        \"\"\"\n        self.content = content\n        self.file_path = None\n        self.cache = TTLCache(maxsize=100, ttl=cache_ttl)\n        self.vision_mode = False\n        self.max_image_size = None  # Changed to None by default\n        self.is_url = False  # Indicates if the source is a URL\n        self.screenshot_timeout = screenshot_timeout\n\n    def set_max_image_size(self, size: int) -&gt; None:\n        \"\"\"Set the maximum image size.\"\"\"\n        self.max_image_size = size\n\n    def set_vision_mode(self, enabled: bool = True) -&gt; None:\n        \"\"\"Enable or disable vision mode processing.\"\"\"\n        self.vision_mode = enabled\n\n    def set_screenshot_timeout(self, timeout: int) -&gt; None:\n        \"\"\"Set the screenshot timeout in milliseconds for capturing a screenshot from a URL.\"\"\"\n        self.screenshot_timeout = timeout\n\n    def can_handle(self, source: Union[str, BytesIO]) -&gt; bool:\n        \"\"\"\n        Checks if the loader can handle the given source.\n\n        Args:\n            source: Either a file path (str) or a BytesIO stream\n\n        Returns:\n            bool: True if the loader can handle the source, False otherwise\n        \"\"\"\n        try:\n            if isinstance(source, str):\n                return self._can_handle_file_path(source)\n            elif isinstance(source, BytesIO):\n                return self._can_handle_stream(source)\n            return False\n        except Exception:\n            return False\n\n    def _can_handle_file_path(self, file_path: str) -&gt; bool:\n        \"\"\"Checks if the loader can handle the given file path.\"\"\"\n        if not os.path.isfile(file_path):\n            return False\n        file_type = get_file_extension(file_path)\n        return file_type.lower() in [fmt.lower() for fmt in self.SUPPORTED_FORMATS]\n\n    def _can_handle_stream(self, stream: BytesIO) -&gt; bool:\n        \"\"\"Checks if the loader can handle the given BytesIO stream.\"\"\"\n        try:\n            mime = magic.from_buffer(stream.getvalue(), mime=True)\n            stream.seek(0)  # Reset stream position\n            return check_mime_type(mime, self.SUPPORTED_FORMATS)\n        except Exception:\n            return False\n\n    @abstractmethod\n    def load(self, source: Union[str, BytesIO]) -&gt; Any:\n        \"\"\"Enhanced load method that handles vision mode.\"\"\"\n        pass\n\n    def getContent(self) -&gt; Any:\n        return self.content\n\n    def convert_to_images(self, file: Union[str, io.BytesIO, io.BufferedReader], scale: float = 300 / 72) -&gt; Dict[int, bytes]:\n        # Determine if the input is a file path or a stream\n        if isinstance(file, str):\n            return self._convert_file_to_images(file, scale)\n        elif isinstance(file, (io.BytesIO, io.BufferedReader)):  # Accept both BytesIO and BufferedReader\n            return self._convert_stream_to_images(file, scale)\n        else:\n            raise TypeError(\"file must be a file path (str) or a file-like stream\")\n\n    def _convert_file_to_images(self, file_path: str, scale: float) -&gt; Dict[int, bytes]:\n        \"\"\"Convert file to images, handling both URLs and local files.\"\"\"\n        # Check if it's a URL\n        if self._is_url(file_path):\n            self.is_url = True  # Set the instance variable if the source is a URL\n            try:\n                screenshot = self._capture_screenshot_from_url(file_path)\n                # Convert screenshot to PIL Image for potential resizing\n                img = Image.open(BytesIO(screenshot))\n                img = self._resize_if_needed(img)\n\n                # Split into vertical chunks\n                chunks = self._split_image_vertically(img)\n\n                # Return dictionary with chunks as list\n                return {0: chunks}  # All chunks from URL are considered \"page 0\"\n\n            except Exception as e:\n                raise ValueError(f\"Failed to capture screenshot from URL: {str(e)}\")\n\n        # Existing code for local files...\n        try:\n            Image.open(file_path)\n            is_image = True\n        except IOError:\n            is_image = False\n\n        if is_image:\n            with open(file_path, \"rb\") as f:\n                return {0: f.read()}\n\n        return self._convert_pdf_to_images(pdfium.PdfDocument(file_path), scale)\n\n    def _convert_stream_to_images(self, file_stream: io.BytesIO, scale: float) -&gt; Dict[int, bytes]:\n        # Check if the stream is already an image\n        try:\n            Image.open(file_stream)\n            is_image = True\n        except IOError:\n            is_image = False\n\n        # Reset stream position\n        file_stream.seek(0)\n\n        if is_image:\n            # If it is, return it as is\n            return {0: file_stream.read()}\n\n        # If it's not an image, proceed with the conversion\n        return self._convert_pdf_to_images(pdfium.PdfDocument(file_stream), scale)\n\n    def _resize_if_needed(self, image: Image.Image) -&gt; Image.Image:\n        \"\"\"Resize image if it exceeds maximum dimensions while maintaining aspect ratio.\n\n        Args:\n            image: PIL Image object\n\n        Returns:\n            PIL Image object (resized if necessary)\n        \"\"\"\n        if self.max_image_size is None:  # Skip resizing if max_image_size not set\n            return image\n\n        width, height = image.size\n        if width &gt; self.max_image_size or height &gt; self.max_image_size:\n            # Calculate scaling factor to fit within max dimensions\n            scale = self.max_image_size / max(width, height)\n            new_width = int(width * scale)\n            new_height = int(height * scale)\n            return image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n        return image\n\n    def _convert_pdf_to_images(self, pdf_file, scale: float) -&gt; Dict[int, bytes]:\n        # Get all pages at once\n        renderer = pdf_file.render(\n            pdfium.PdfBitmap.to_pil,\n            page_indices=list(range(len(pdf_file))),\n            scale=scale,\n        )\n\n        # Convert all images to bytes and store in dictionary\n        final_images = {}\n        for page_index, image in enumerate(renderer):\n            # Resize image if needed\n            image = self._resize_if_needed(image)\n            image_byte_array = BytesIO()\n            image.save(image_byte_array, format=\"jpeg\", optimize=True)\n            final_images[page_index] = image_byte_array.getvalue()\n\n        return final_images\n\n    def can_handle_vision(self, source: Union[str, BytesIO]) -&gt; bool:\n        \"\"\"\n        Checks if the loader can handle the source in vision mode.\n\n        Args:\n            source: Either a file path (str), URL, or a BytesIO stream\n\n        Returns:\n            bool: True if the loader can handle the source in vision mode\n        \"\"\"\n        try:\n            if isinstance(source, str):\n                if self._is_url(source):\n                    return True  # URLs are always supported in vision mode\n                ext = get_file_extension(source).lower()\n                return ext in ['pdf', 'jpg', 'jpeg', 'png', 'tiff', 'bmp']\n            elif isinstance(source, BytesIO):\n                try:\n                    Image.open(source)\n                    return True\n                except:\n                    # Try to load as PDF\n                    try:\n                        pdfium.PdfDocument(source)\n                        return True\n                    except:\n                        return False\n            return False\n        except Exception:\n            return False\n\n    def can_handle_paginate(self, source: Union[str, BytesIO]) -&gt; bool:\n        \"\"\"\n        Checks if the source supports pagination (e.g., PDF, PPT).\n\n        Args:\n            source: Either a file path (str) or a BytesIO stream\n\n        Returns:\n            bool: True if the source supports pagination\n        \"\"\"\n        try:\n            if isinstance(source, str):\n                # For file paths, check the extension\n                ext = get_file_extension(source).lower()\n            else:\n                # For BytesIO streams, use magic to detect mime type\n                mime = magic.from_buffer(source.getvalue(), mime=True)\n                source.seek(0)  # Reset stream position\n                return mime == 'application/pdf'\n\n            # List of extensions that support pagination\n            return ext in ['pdf']\n        except Exception:\n            return False\n\n    @staticmethod\n    def _check_playwright_dependencies():\n        \"\"\"\n        Check if the playwright dependency is installed.\n        Raises:\n            ImportError: If playwright is not installed.\n        \"\"\"\n        try:\n            from playwright.sync_api import sync_playwright\n        except ImportError:\n            raise ImportError(\n                \"You are using vision with url. You need to install playwright.\"\n                \"`pip install playwright` and run `playwright install`.\"\n            )\n\n    def _capture_screenshot_from_url(self, url: str) -&gt; bytes:\n        \"\"\"\n        Captures a full-page screenshot of a URL using Playwright.\n\n        Args:\n            url: The URL to capture\n\n        Returns:\n            bytes: The screenshot image data\n        \"\"\"\n        # Optional: Check if playwright is installed before attempting to use it.\n        self._check_playwright_dependencies()\n\n        from playwright.sync_api import sync_playwright  # Import after the dependency check\n\n        with sync_playwright() as p:\n            browser = p.chromium.launch(headless=True)\n            page = browser.new_page()\n\n            try:\n                # Navigate to URL\n                page.goto(url, wait_until='networkidle')\n\n                # Optional: Handle cookie consent popups (customize selectors as needed)\n                try:\n                    page.click('button:has-text(\"Accept\")', timeout=10000)\n                except Exception:\n                    pass  # Ignore if no cookie banner is found\n\n                # Wait for content to load with the configurable timeout\n                page.wait_for_timeout(self.screenshot_timeout)\n\n                # Capture full page screenshot\n                screenshot = page.screenshot(full_page=True)\n\n                return screenshot\n\n            finally:\n                browser.close()\n\n    def _split_image_vertically(self, img: Image.Image, chunk_height: int = 1000) -&gt; List[bytes]:\n        \"\"\"\n        Splits a tall PIL Image into vertical chunks of `chunk_height`.\n        Returns a list of bytes in PNG format, in top-to-bottom order.\n\n        Args:\n            img: PIL Image to split\n            chunk_height: Height of each chunk in pixels\n\n        Returns:\n            List of PNG-encoded bytes for each chunk\n        \"\"\"\n        width, height = img.size\n        num_chunks = math.ceil(height / chunk_height)\n\n        chunks_bytes = []\n        for i in range(num_chunks):\n            top = i * chunk_height\n            bottom = min((i + 1) * chunk_height, height)\n            crop_box = (0, top, width, bottom)\n\n            # Crop the chunk\n            chunk_img = img.crop(crop_box)\n\n            # Convert chunk to bytes\n            chunk_bytes = io.BytesIO()\n            chunk_img.save(chunk_bytes, format=\"PNG\", optimize=True)\n            chunk_bytes.seek(0)\n            chunks_bytes.append(chunk_bytes.read())\n\n        return chunks_bytes\n\n    def _is_url(self, source: str) -&gt; bool:\n        \"\"\"Check if the source string is a URL.\"\"\"\n        try:\n            result = urlparse(source)\n            return bool(result.scheme and result.netloc)\n        except:\n            return False\n</code></pre></p>"},{"location":"core-concepts/document-loaders/#core-features","title":"Core Features","text":""},{"location":"core-concepts/document-loaders/#configuration-support","title":"Configuration Support","text":"<p>All Document Loaders support configuration-based initialization through dedicated config classes:</p> <pre><code>from extract_thinker import DocumentLoaderAWSTextract, TextractConfig\n\n# Create configuration\nconfig = TextractConfig(\n    aws_access_key_id=\"your_key\",\n    feature_types=[\"TABLES\", \"FORMS\"],\n    cache_ttl=600\n)\n\n# Initialize with configuration\nloader = DocumentLoaderAWSTextract(config)\n</code></pre>"},{"location":"core-concepts/document-loaders/#caching","title":"Caching","text":"<p>All Document Loaders include built-in caching capabilities through the <code>CachedDocumentLoader</code> base class. This provides automatic caching of document processing results with a configurable TTL:</p> Cached Document Loader <p>The CachedDocumentLoader extends the base loader with caching capabilities: <pre><code>from io import BytesIO\nfrom typing import Any, Union\nfrom cachetools import TTLCache\nfrom extract_thinker.document_loader.document_loader import DocumentLoader\n\n\nclass CachedDocumentLoader(DocumentLoader):\n    def __init__(self, content: Any = None, cache_ttl: int = 300):\n        super().__init__(content)\n        self.cache = TTLCache(maxsize=100, ttl=cache_ttl)\n\n    def load(self, source: Union[str, BytesIO]) -&gt; Any:\n        \"\"\"\n        Load content from source with caching support.\n\n        Args:\n            source: Either a file path (str) or a BytesIO stream\n\n        Returns:\n            The loaded content\n        \"\"\"\n        # Use the source and vision_mode state as the cache key\n        if isinstance(source, str):\n            cache_key = (source, self.vision_mode)\n        else:\n            # For BytesIO, use the content and vision_mode state as the cache key\n            cache_key = (source.getvalue(), self.vision_mode)\n\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n\n        result = super().load(source)\n        self.cache[cache_key] = result\n        return result\n</code></pre></p> <p>Example usage of caching: <pre><code>from extract_thinker.document_loader import DocumentLoader\n\nclass MyCustomLoader(DocumentLoader):\n    def __init__(self, content: Any = None, cache_ttl: int = 300):\n        super().__init__(content, cache_ttl)  # 300 seconds default TTL\n</code></pre></p>"},{"location":"core-concepts/document-loaders/#file-type-support","title":"File Type Support","text":"<p>Document Loaders automatically validate file types through the <code>can_handle</code> method:</p> <pre><code>loader = MyCustomLoader()\nif loader.can_handle(\"document.pdf\"):\n    content = loader.load(\"document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/#multiple-input-types","title":"Multiple Input Types","text":"<p>Loaders support both file paths and BytesIO streams:</p> <pre><code># Load from file\ncontent = loader.load(\"document.pdf\")\n\n# Load from stream\nwith open(\"document.pdf\", \"rb\") as f:\n    stream = BytesIO(f.read())\n    content = loader.load(stream)\n</code></pre>"},{"location":"core-concepts/document-loaders/#vision-mode-support","title":"Vision Mode Support","text":"<p>Many loaders support vision mode for handling images and visual content:</p> <pre><code># Enable vision mode\nloader.set_vision_mode(True)\n\n# Load document with images\npages = loader.load(\"document.pdf\")\nfor page in pages:\n    text = page[\"content\"]\n    image = page.get(\"image\")  # Available in vision mode\n</code></pre>"},{"location":"core-concepts/document-loaders/#image-resizing","title":"Image Resizing","text":"<pre><code>loader = DocumentLoader()\nloader.set_max_image_size(2000)\n</code></pre>"},{"location":"core-concepts/document-loaders/#image-conversion","title":"Image Conversion","text":"<p>The base loader includes utilities for converting documents to images:</p> <pre><code>loader = DocumentLoader()\nimages = loader.convert_to_images(\n    \"document.pdf\",\n    scale=300/72  # DPI scaling\n)\n</code></pre>"},{"location":"core-concepts/document-loaders/#common-methods","title":"Common Methods","text":"<p>All Document Loaders implement these core methods:</p> <ul> <li><code>load(source)</code>: Main entry point for loading documents</li> <li><code>set_vision_mode(enabled)</code>: Enable/disable vision mode</li> <li><code>set_max_image_size(size)</code>: Set the maximum image size</li> </ul>"},{"location":"core-concepts/document-loaders/#best-practices","title":"Best Practices","text":"<ul> <li>Use configuration classes for complex initialization</li> <li>Set appropriate cache TTL based on your use case</li> <li>Check file type support before processing</li> <li>Consider memory usage when processing large files</li> <li>Enable vision mode only when needed</li> <li>Handle both file paths and streams for flexibility</li> </ul>"},{"location":"core-concepts/document-loaders/#available-loaders","title":"Available Loaders","text":"<p>ExtractThinker provides several specialized Document Loaders:</p>"},{"location":"core-concepts/document-loaders/#cloud-services","title":"Cloud Services","text":"<ul> <li>AWS Textract: AWS document processing with support for text, tables, forms, and layout analysis</li> <li>Azure Form: Azure's Document Intelligence with multiple model support</li> <li>Google Document AI: Google's document understanding with native PDF parsing</li> </ul>"},{"location":"core-concepts/document-loaders/#local-processing","title":"Local Processing","text":"<ul> <li>PDF Plumber: Advanced PDF text and table extraction</li> <li>PyPDF: Basic PDF processing with password protection support</li> <li>Tesseract: Open-source OCR with multiple language support</li> <li>Doc2txt: Microsoft Word document processing</li> <li>Spreadsheet: Excel and CSV handling</li> <li>Text File: Plain text file handling with encoding support</li> <li>Markitdown: Multi-format document processing</li> <li>Docling: Advanced document layout and table analysis</li> </ul>"},{"location":"core-concepts/document-loaders/#special-purpose","title":"Special Purpose","text":"<ul> <li>Web Loader: Web page extraction with custom element handling</li> <li>LLM Image: Vision-enabled LLM processing</li> <li>Data: Pre-processed data handling with standardized format support</li> </ul>"},{"location":"core-concepts/document-loaders/#coming-soon","title":"Coming Soon","text":"<ul> <li><code>Adobe PDF Services</code> Coming Soon: Adobe's PDF extraction and analysis</li> <li><code>ABBYY FineReader</code> Coming Soon: Enterprise-grade OCR solution</li> <li><code>PaddleOCR</code> Coming Soon: High-performance multilingual OCR</li> <li><code>Unstructured</code> Coming Soon: Open-source document preprocessing</li> <li><code>Mathpix</code> Coming Soon: Math and scientific document processing</li> <li><code>EasyOCR</code> Coming Soon: Ready-to-use OCR with multilingual support</li> <li><code>Nanonets</code> Coming Soon: API-based document processing</li> <li><code>Mindee</code> Coming Soon: Specialized document parsing APIs</li> <li><code>Rossum</code> Coming Soon: AI-powered document understanding</li> <li><code>Kofax</code> Coming Soon: Intelligent document processing</li> </ul>"},{"location":"core-concepts/document-loaders/aws-textract/","title":"AWS Textract Document Loader","text":"<p>The AWS Textract loader uses Amazon's Textract service to extract text, forms, and tables from documents. It supports both image files and PDFs.</p>"},{"location":"core-concepts/document-loaders/aws-textract/#supported-formats","title":"Supported Formats","text":"<ul> <li>pdf</li> <li>jpeg</li> <li>png</li> <li>tiff</li> </ul>"},{"location":"core-concepts/document-loaders/aws-textract/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/aws-textract/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderAWSTextract\n\n# Initialize with AWS credentials\nloader = DocumentLoaderAWSTextract(\n    aws_access_key_id=\"your_access_key\",\n    aws_secret_access_key=\"your_secret_key\",\n    region_name=\"your_region\"\n)\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access tables if extracted\n    tables = page.get(\"tables\", [])\n</code></pre>"},{"location":"core-concepts/document-loaders/aws-textract/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderAWSTextract, TextractConfig\n\n# Create configuration\nconfig = TextractConfig(\n    aws_access_key_id=\"your_access_key\",\n    aws_secret_access_key=\"your_secret_key\",\n    region_name=\"your_region\",\n    feature_types=[\"TABLES\", \"FORMS\", \"SIGNATURES\"],  # Specify features to extract\n    cache_ttl=600,                                    # Cache results for 10 minutes\n    max_retries=3                                     # Number of retry attempts\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderAWSTextract(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/aws-textract/#configuration-options","title":"Configuration Options","text":"<p>The <code>TextractConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>aws_access_key_id</code> str None AWS access key ID <code>aws_secret_access_key</code> str None AWS secret access key <code>region_name</code> str None AWS region name <code>textract_client</code> boto3.client None Pre-configured Textract client <code>feature_types</code> List[str] [] Features to extract (TABLES, FORMS, LAYOUT, SIGNATURES) <code>max_retries</code> int 3 Maximum number of retry attempts"},{"location":"core-concepts/document-loaders/aws-textract/#features","title":"Features","text":"<ul> <li>Text extraction from images and PDFs</li> <li>Table detection and extraction</li> <li>Form field detection</li> <li>Layout analysis</li> <li>Signature detection</li> <li>Configurable feature selection</li> <li>Automatic retry on failure</li> <li>Caching support</li> <li>Support for pre-configured clients</li> </ul>"},{"location":"core-concepts/document-loaders/aws-textract/#notes","title":"Notes","text":"<ul> <li>Raw text extraction is the default when no feature types are specified</li> <li>\"QUERIES\" feature type is not supported</li> <li>Vision mode is supported for image formats</li> <li>AWS credentials are required unless using a pre-configured client</li> <li>Rate limits and quotas apply based on your AWS account </li> </ul>"},{"location":"core-concepts/document-loaders/azure-form/","title":"Azure Document Intelligence Loader","text":"<p>The Azure Document Intelligence loader (formerly Form Recognizer) uses Azure's Document Intelligence service to extract text, tables, and structured information from documents.</p>"},{"location":"core-concepts/document-loaders/azure-form/#supported-formats","title":"Supported Formats","text":"<p>Supports <code>PDF</code>, <code>JPEG/JPG</code>, <code>PNG</code>, <code>BMP</code>, <code>TIFF</code>, <code>HEIF</code>, <code>DOCX</code>, <code>XLSX</code>, <code>PPTX</code> and <code>HTML</code>.</p>"},{"location":"core-concepts/document-loaders/azure-form/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/azure-form/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderAzureForm\n\n# Initialize with Azure credentials\nloader = DocumentLoaderAzureForm(\n    endpoint=\"your_endpoint\",\n    key=\"your_api_key\",\n    model=\"prebuilt-document\"  # Use prebuilt document model\n)\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access tables if available\n    tables = page.get(\"tables\", [])\n</code></pre>"},{"location":"core-concepts/document-loaders/azure-form/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderAzureForm, AzureConfig\n\n# Create configuration\nconfig = AzureConfig(\n    endpoint=\"your_endpoint\",\n    key=\"your_api_key\",\n    model=\"prebuilt-read\",     # Use layout model for enhanced layout analysis\n    language=\"en\",               # Specify document language\n    pages=[1, 2, 3],            # Process specific pages\n    cache_ttl=600               # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderAzureForm(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/azure-form/#configuration-options","title":"Configuration Options","text":"<p>The <code>AzureConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>endpoint</code> str None Azure endpoint URL <code>key</code> str None Azure API key <code>model</code> str \"prebuilt-document\" Model ID to use <code>language</code> str None Document language code <code>pages</code> List[int] None Specific pages to process <code>reading_order</code> str \"natural\" Text reading order"},{"location":"core-concepts/document-loaders/azure-form/#features","title":"Features","text":"<ul> <li>Text extraction with layout preservation</li> <li>Table detection and extraction</li> <li>Form field recognition</li> <li>Multiple model support (document, layout, read)</li> <li>Language specification</li> <li>Page selection</li> <li>Reading order control</li> <li>Caching support</li> <li>Support for pre-configured clients</li> </ul>"},{"location":"core-concepts/document-loaders/azure-form/#notes","title":"Notes","text":"<ul> <li>Available models: \"prebuilt-document\", \"prebuilt-layout\", \"prebuilt-read\"</li> <li>Vision mode is supported for image formats</li> <li>Azure credentials are required</li> <li>Rate limits and quotas apply based on your Azure subscription</li> </ul>"},{"location":"core-concepts/document-loaders/data/","title":"Data Document Loader","text":"<p>The Data loader is a specialized loader that handles pre-processed data in a standardized format. It provides caching support and vision mode compatibility.</p>"},{"location":"core-concepts/document-loaders/data/#supported-format","title":"Supported Format","text":"<p>The loader expects data in the following standard format: <pre><code>[\n  {\n    \"content\": \"...some text...\",\n    \"image\": None or [] or bytes\n  }\n]\n</code></pre></p>"},{"location":"core-concepts/document-loaders/data/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/data/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderData\n\n# Initialize with default settings\nloader = DocumentLoaderData()\n\n# Load pre-formatted data\ndata = [{\"content\": \"Sample text\", \"image\": None}]\npages = loader.load(data)\n\n# Process content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access image data if present\n    image = page[\"image\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/data/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderData, DataLoaderConfig\n\n# Create configuration\nconfig = DataLoaderConfig(\n    content=None,                # Initial content\n    cache_ttl=600,              # Cache results for 10 minutes\n    supports_vision=True         # Enable vision support\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderData(config)\n\n# Load and process content\npages = loader.load(\"raw text content\")\n</code></pre>"},{"location":"core-concepts/document-loaders/data/#configuration-options","title":"Configuration Options","text":"<p>The <code>DataLoaderConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>supports_vision</code> bool True Whether vision mode is supported"},{"location":"core-concepts/document-loaders/doc2txt/","title":"Doc2txt Document Loader","text":"<p>The Doc2txt loader extracts text from Microsoft Word documents. It supports both legacy (.doc) and modern (.docx) file formats.</p>"},{"location":"core-concepts/document-loaders/doc2txt/#supported-formats","title":"Supported Formats","text":"<ul> <li>doc</li> <li>docx</li> </ul>"},{"location":"core-concepts/document-loaders/doc2txt/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/doc2txt/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderDoc2txt\n\n# Initialize with default settings\nloader = DocumentLoaderDoc2txt()\n\n# Load document\npages = loader.load(\"path/to/your/document.docx\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/doc2txt/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderDoc2txt, Doc2txtConfig\n\n# Create configuration\nconfig = Doc2txtConfig(\n    page_separator=\"\\n\\n---\\n\\n\",  # Custom page separator\n    preserve_whitespace=True,      # Preserve original whitespace\n    extract_images=True,           # Extract embedded images\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderDoc2txt(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.docx\")\n</code></pre>"},{"location":"core-concepts/document-loaders/doc2txt/#configuration-options","title":"Configuration Options","text":"<p>The <code>Doc2txtConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>page_separator</code> str \"\\n\\n\" Text to use as page separator <code>preserve_whitespace</code> bool False Whether to preserve whitespace <code>extract_images</code> bool False Whether to extract embedded images"},{"location":"core-concepts/document-loaders/doc2txt/#features","title":"Features","text":"<ul> <li>Text extraction from Word documents</li> <li>Support for both .doc and .docx</li> <li>Custom page separation</li> <li>Whitespace preservation</li> <li>Image extraction (optional)</li> <li>Caching support</li> <li>No cloud service required</li> </ul>"},{"location":"core-concepts/document-loaders/doc2txt/#notes","title":"Notes","text":"<ul> <li>Vision mode is not supported</li> <li>Image extraction requires additional memory</li> <li>Local processing with no external dependencies</li> <li>May not preserve complex formatting</li> <li>Handles both legacy and modern Word formats</li> </ul>"},{"location":"core-concepts/document-loaders/docling/","title":"Docling Document Loader","text":"<p>The Docling loader is a specialized document processor that excels at handling complex document layouts and table structures. It provides advanced OCR capabilities and precise table detection.</p>"},{"location":"core-concepts/document-loaders/docling/#supported-formats","title":"Supported Formats","text":""},{"location":"core-concepts/document-loaders/docling/#documents","title":"Documents","text":"<ul> <li>pdf</li> <li>doc/docx</li> <li>ppt/pptx</li> <li>xls/xlsx</li> </ul>"},{"location":"core-concepts/document-loaders/docling/#images","title":"Images","text":"<ul> <li>jpeg/jpg</li> <li>png</li> <li>tiff</li> <li>bmp</li> <li>gif</li> <li>webp</li> </ul>"},{"location":"core-concepts/document-loaders/docling/#text","title":"Text","text":"<ul> <li>txt</li> <li>html</li> <li>xml</li> <li>json</li> </ul>"},{"location":"core-concepts/document-loaders/docling/#others","title":"Others","text":"<ul> <li>csv</li> <li>tsv</li> <li>zip</li> </ul>"},{"location":"core-concepts/document-loaders/docling/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/docling/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderDocling\n\n# Initialize with default settings\nloader = DocumentLoaderDocling()\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access tables if available\n    tables = page.get(\"tables\", [])\n</code></pre>"},{"location":"core-concepts/document-loaders/docling/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderDocling, DoclingConfig\n\n# Create configuration\nconfig = DoclingConfig(\n    ocr_enabled=True,                # Enable OCR processing\n    table_structure_enabled=True,    # Enable table structure detection\n    tesseract_cmd=\"path/to/tesseract\", # Custom Tesseract path\n    force_full_page_ocr=False,      # Use selective OCR\n    do_cell_matching=True,          # Enable cell content matching\n    format_options={                # Format-specific options\n        \"pdf\": {\"dpi\": 300},\n        \"image\": {\"enhance\": True}\n    },\n    cache_ttl=600                   # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderDocling(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/docling/#configuration-options","title":"Configuration Options","text":"<p>The <code>DoclingConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>ocr_enabled</code> bool False Enable OCR processing <code>table_structure_enabled</code> bool True Enable table structure detection <code>tesseract_cmd</code> str None Path to Tesseract executable <code>force_full_page_ocr</code> bool False Force OCR on entire page <code>do_cell_matching</code> bool True Enable cell content matching <code>format_options</code> Dict None Format-specific processing options"},{"location":"core-concepts/document-loaders/docling/#features","title":"Features","text":"<ul> <li>Advanced table structure detection</li> <li>Selective OCR processing</li> <li>Cell content matching</li> <li>Format-specific optimizations</li> <li>Custom Tesseract integration</li> <li>Table content deduplication</li> <li>Multi-format support</li> <li>Caching support</li> <li>Stream-based loading</li> </ul>"},{"location":"core-concepts/document-loaders/docling/#notes","title":"Notes","text":"<ul> <li>Vision mode is supported for image formats</li> <li>OCR requires Tesseract installation</li> <li>Table detection works best with structured documents</li> <li>Performance depends on document complexity</li> <li>Handles both scanned and digital documents</li> <li>Supports multiple document formats through format-specific optimizations </li> </ul>"},{"location":"core-concepts/document-loaders/google-document-ai/","title":"Google Document AI Loader","text":"<p>The Google Document AI loader uses Google Cloud's Document AI service to extract text, tables, and structured information from documents.</p>"},{"location":"core-concepts/document-loaders/google-document-ai/#supported-formats","title":"Supported Formats","text":"<ul> <li>pdf</li> <li>jpeg/jpg</li> <li>png</li> <li>tiff</li> <li>gif</li> <li>bmp</li> <li>webp</li> </ul>"},{"location":"core-concepts/document-loaders/google-document-ai/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/google-document-ai/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderGoogleDocumentAI\n\n# Initialize with Google Cloud credentials\nloader = DocumentLoaderGoogleDocumentAI(\n    project_id=\"your_project_id\",\n    location=\"your_location\",\n    processor_id=\"your_processor_id\",\n    credentials_path=\"path/to/credentials.json\"\n)\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access tables if available\n    tables = page.get(\"tables\", [])\n</code></pre>"},{"location":"core-concepts/document-loaders/google-document-ai/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderGoogleDocumentAI, GoogleDocAIConfig\n\n# Create configuration\nconfig = GoogleDocAIConfig(\n    project_id=\"your_project_id\",\n    location=\"your_location\",\n    processor_id=\"your_processor_id\",\n    credentials_path=\"path/to/credentials.json\",\n    mime_type=\"application/pdf\",    # Specify MIME type\n    process_options={               # Additional processing options\n        \"ocr_config\": {\"enable_native_pdf_parsing\": True}\n    },\n    cache_ttl=600                   # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderGoogleDocumentAI(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/google-document-ai/#configuration-options","title":"Configuration Options","text":"<p>The <code>GoogleDocAIConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>project_id</code> str None Google Cloud project ID <code>location</code> str None Processing location <code>processor_id</code> str None Document AI processor ID <code>credentials_path</code> str None Path to credentials file <code>credentials</code> Credentials None Pre-configured credentials <code>mime_type</code> str None Document MIME type <code>process_options</code> Dict None Additional processing options"},{"location":"core-concepts/document-loaders/google-document-ai/#features","title":"Features","text":"<ul> <li>Text extraction with layout preservation</li> <li>Table detection and extraction</li> <li>Form field recognition</li> <li>Multiple processor support</li> <li>Native PDF parsing</li> <li>Custom processing options</li> <li>Caching support</li> <li>Support for pre-configured credentials</li> </ul>"},{"location":"core-concepts/document-loaders/google-document-ai/#notes","title":"Notes","text":"<ul> <li>Vision mode is supported for image formats</li> <li>Google Cloud credentials are required</li> <li>Rate limits and quotas apply based on your Google Cloud account</li> <li>Different processors may support different document types</li> <li>Native PDF parsing can improve performance for PDF documents</li> </ul>"},{"location":"core-concepts/document-loaders/llm-image/","title":"LLM Image Document Loader","text":"<p>The LLM Image loader is a specialized loader designed to handle images and PDFs for vision-enabled Language Models. It serves as a fallback loader when no other loader is available and vision mode is required.</p>"},{"location":"core-concepts/document-loaders/llm-image/#supported-formats","title":"Supported Formats","text":"<ul> <li>jpeg/jpg</li> <li>png</li> <li>gif</li> <li>bmp</li> <li>webp</li> <li>tiff</li> </ul>"},{"location":"core-concepts/document-loaders/llm-image/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/llm-image/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderLLMImage\n\n# Initialize with default settings\nloader = DocumentLoaderLLMImage()\n\n# Load document\npages = loader.load(\"path/to/your/image.jpg\")\n\n# Process extracted content\nfor page in pages:\n    # Access image content\n    image_bytes = page[\"image\"]\n    # Access metadata if available\n    metadata = page.get(\"metadata\", {})\n</code></pre>"},{"location":"core-concepts/document-loaders/llm-image/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderLLMImage, LLMImageConfig\n\n# Create configuration\nconfig = LLMImageConfig(\n    max_image_size=1024 * 1024,    # Maximum image size in bytes\n    image_format=\"jpeg\",           # Target image format\n    compression_quality=85,        # JPEG compression quality\n    llm=\"gpt-4-vision\",           # Target LLM model\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderLLMImage(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/image.jpg\")\n</code></pre>"},{"location":"core-concepts/document-loaders/llm-image/#configuration-options","title":"Configuration Options","text":"<p>The <code>LLMImageConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>llm</code> str None Target LLM model <code>max_image_size</code> int 1048576 Maximum image size in bytes <code>image_format</code> str \"jpeg\" Target image format <code>compression_quality</code> int 85 JPEG compression quality"},{"location":"core-concepts/document-loaders/llm-image/#features","title":"Features","text":"<ul> <li>Processing documents where text extraction is difficult or unreliable</li> <li>Working with image-heavy documents</li> <li>Using vision-enabled LLMs for document understanding</li> <li>Fallback option when other loaders fail</li> </ul>"},{"location":"core-concepts/document-loaders/llm-image/#notes","title":"Notes","text":"<ul> <li>This loader is specifically designed for vision/image processing</li> <li>It doesn't extract text content (content field will be empty)</li> <li>Each page will contain the image data in the 'image' field</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/","title":"MarkItDown Document Loader","text":"<p>MarkItDown is a versatile document processing library from Microsoft that can handle multiple file formats. The MarkItDown loader provides a robust interface for text extraction with optional vision mode support.</p>"},{"location":"core-concepts/document-loaders/markitdown/#supported-formats","title":"Supported Formats","text":""},{"location":"core-concepts/document-loaders/markitdown/#documents","title":"Documents","text":"<ul> <li>pdf</li> <li>doc/docx</li> <li>ppt/pptx</li> <li>xls/xlsx</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#text","title":"Text","text":"<ul> <li>txt</li> <li>html</li> <li>xml</li> <li>json</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#images","title":"Images","text":"<ul> <li>jpg/jpeg</li> <li>png</li> <li>bmp</li> <li>gif</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#audio","title":"Audio","text":"<ul> <li>wav</li> <li>mp3</li> <li>m4a</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#others","title":"Others","text":"<ul> <li>csv</li> <li>tsv</li> <li>zip</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/markitdown/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderMarkItDown\n\n# Initialize with default settings\nloader = DocumentLoaderMarkItDown()\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/markitdown/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderMarkItDown, MarkItDownConfig\n\n# Create configuration\nconfig = MarkItDownConfig(\n    page_separator=\"---\",          # Custom page separator\n    preserve_whitespace=True,      # Preserve original whitespace\n    mime_type_detection=True,      # Enable MIME type detection\n    default_extension=\".md\",       # Default file extension\n    llm_client=\"gpt-4\",           # LLM client for enhanced parsing\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderMarkItDown(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.md\")\n</code></pre>"},{"location":"core-concepts/document-loaders/markitdown/#configuration-options","title":"Configuration Options","text":"<p>The <code>MarkItDownConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>page_separator</code> str \"\\n\\n\" Text to use as page separator <code>preserve_whitespace</code> bool False Whether to preserve whitespace <code>mime_type_detection</code> bool True Enable MIME type detection <code>default_extension</code> str \".txt\" Default file extension <code>llm_client</code> str None LLM client for enhanced parsing <code>llm_model</code> str None LLM model for enhanced parsing"},{"location":"core-concepts/document-loaders/markitdown/#features","title":"Features","text":"<ul> <li>Multi-format document processing</li> <li>Text and layout preservation</li> <li>MIME type detection</li> <li>Custom page separation</li> <li>Whitespace preservation</li> <li>LLM-enhanced parsing</li> <li>Caching support</li> <li>Stream-based loading</li> </ul>"},{"location":"core-concepts/document-loaders/markitdown/#notes","title":"Notes","text":"<ul> <li>Vision mode is supported for image formats</li> <li>LLM enhancement is optional</li> <li>Local processing with no external dependencies</li> <li>Preserves document structure</li> <li>Handles a wide variety of file formats</li> </ul>"},{"location":"core-concepts/document-loaders/pdf-plumber/","title":"PDFPlumber Document Loader","text":"<p>The PDFPlumber loader uses the pdfplumber library to extract text and tables from PDF documents with precise layout preservation.</p>"},{"location":"core-concepts/document-loaders/pdf-plumber/#supported-formats","title":"Supported Formats","text":"<ul> <li>pdf</li> </ul>"},{"location":"core-concepts/document-loaders/pdf-plumber/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/pdf-plumber/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderPdfPlumber\n\n# Initialize with default settings\nloader = DocumentLoaderPdfPlumber()\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n    # Access tables if extracted\n    tables = page.get(\"tables\", [])\n</code></pre>"},{"location":"core-concepts/document-loaders/pdf-plumber/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderPdfPlumber, PDFPlumberConfig\n\n# Create configuration\nconfig = PDFPlumberConfig(\n    table_settings={                # Custom table extraction settings\n        \"vertical_strategy\": \"text\",\n        \"horizontal_strategy\": \"lines\",\n        \"intersection_y_tolerance\": 10\n    },\n    vision_enabled=True,           # Enable vision mode for images\n    extract_tables=True,           # Enable table extraction\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderPdfPlumber(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/pdf-plumber/#configuration-options","title":"Configuration Options","text":"<p>The <code>PDFPlumberConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>table_settings</code> Dict None Custom table extraction settings <code>vision_enabled</code> bool False Enable vision mode for images <code>extract_tables</code> bool True Enable table extraction"},{"location":"core-concepts/document-loaders/pdf-plumber/#features","title":"Features","text":"<ul> <li>Text extraction with layout preservation</li> <li>Table detection and extraction</li> <li>Custom table extraction settings</li> <li>Vision mode support</li> <li>Precise positioning information</li> <li>Caching support</li> <li>No cloud service required</li> </ul>"},{"location":"core-concepts/document-loaders/pdf-plumber/#notes","title":"Notes","text":"<ul> <li>Vision mode can be enabled for image extraction</li> <li>Table extraction can be disabled for better performance</li> <li>Custom table settings can improve extraction accuracy</li> <li>Local processing with no external dependencies</li> </ul>"},{"location":"core-concepts/document-loaders/pypdf/","title":"PyPDF Document Loader","text":"<p>The PyPDF loader uses the PyPDF library to extract text and images from PDF documents. It provides basic text extraction and supports password-protected PDFs.</p>"},{"location":"core-concepts/document-loaders/pypdf/#supported-formats","title":"Supported Formats","text":"<ul> <li>pdf</li> </ul>"},{"location":"core-concepts/document-loaders/pypdf/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/pypdf/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderPyPdf\n\n# Initialize with default settings\nloader = DocumentLoaderPyPdf()\n\n# Load document\npages = loader.load(\"path/to/your/document.pdf\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/pypdf/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderPyPdf, PyPDFConfig\n\n# Create configuration\nconfig = PyPDFConfig(\n    password=\"your_password\",      # For password-protected PDFs\n    vision_enabled=True,           # Enable vision mode for images\n    extract_text=True,             # Enable text extraction\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderPyPdf(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.pdf\")\n</code></pre>"},{"location":"core-concepts/document-loaders/pypdf/#configuration-options","title":"Configuration Options","text":"<p>The <code>PyPDFConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>password</code> str None Password for protected PDFs <code>vision_enabled</code> bool False Enable vision mode for images <code>extract_text</code> bool True Enable text extraction"},{"location":"core-concepts/document-loaders/pypdf/#features","title":"Features","text":"<ul> <li>Basic text extraction</li> <li>Password-protected PDF support</li> <li>Image extraction (with vision mode)</li> <li>Caching support</li> <li>No cloud service required</li> <li>Lightweight and fast processing</li> </ul>"},{"location":"core-concepts/document-loaders/pypdf/#notes","title":"Notes","text":"<ul> <li>Vision mode can be enabled for image extraction</li> <li>Text extraction can be disabled for better performance</li> <li>Supports encrypted/password-protected PDFs</li> <li>Local processing with no external dependencies</li> <li>May not preserve complex layouts or tables</li> </ul>"},{"location":"core-concepts/document-loaders/spreadsheet/","title":"Spreadsheet Document Loader","text":"<p>The spreadsheet loader is designed to handle various spreadsheet formats including Excel files (xls, xlsx, xlsm, xlsb) and OpenDocument formats (odf, ods, odt).</p>"},{"location":"core-concepts/document-loaders/spreadsheet/#installation","title":"Installation","text":"<p>To use the spreadsheet loader, you need to install the required dependencies:</p> <pre><code>pip install openpyxl xlrd\n</code></pre>"},{"location":"core-concepts/document-loaders/spreadsheet/#supported-formats","title":"Supported Formats","text":"<p><code>xls</code>, <code>xlsx</code>, <code>xlsm</code>, <code>xlsb</code>, <code>odf</code>, <code>ods</code>, <code>odt</code>, <code>csv</code></p>"},{"location":"core-concepts/document-loaders/spreadsheet/#usage","title":"Usage","text":"<pre><code>from extract_thinker import DocumentLoaderSpreadSheet\n\n# Initialize the loader\nloader = DocumentLoaderSpreadSheet()\n\n# Load from file\npages = loader.load(\"path/to/your/spreadsheet.xlsx\")\n\n# Load CSV file\ncsv_content = loader.load_content_from_file(\"data.csv\")\n</code></pre>"},{"location":"core-concepts/document-loaders/spreadsheet/#features","title":"Features","text":"<ul> <li>Excel file support (.xlsx, .xls)</li> <li>CSV file support</li> <li>Multiple sheet handling</li> <li>Data type preservation</li> </ul>"},{"location":"core-concepts/document-loaders/tesseract/","title":"Tesseract Document Loader","text":"<p>The Tesseract loader uses the Tesseract OCR engine to extract text from images. It supports multiple languages and provides various OCR optimization options.</p>"},{"location":"core-concepts/document-loaders/tesseract/#supported-formats","title":"Supported Formats","text":"<ul> <li>jpeg/jpg</li> <li>png</li> <li>tiff</li> <li>bmp</li> <li>gif</li> </ul>"},{"location":"core-concepts/document-loaders/tesseract/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/tesseract/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderTesseract\n\n# Initialize with default settings\nloader = DocumentLoaderTesseract()\n\n# Load document\npages = loader.load(\"path/to/your/image.png\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/tesseract/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderTesseract, TesseractConfig\n\n# Create configuration\nconfig = TesseractConfig(\n    lang=\"eng+fra\",                # Use English and French\n    psm=6,                         # Assume uniform block of text\n    oem=3,                         # Default LSTM OCR Engine Mode\n    config_params={                # Additional Tesseract parameters\n        \"tessedit_char_whitelist\": \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n    },\n    timeout=30,                    # OCR timeout in seconds\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderTesseract(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/image.png\")\n</code></pre>"},{"location":"core-concepts/document-loaders/tesseract/#configuration-options","title":"Configuration Options","text":"<p>The <code>TesseractConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>lang</code> str \"eng\" Language(s) for OCR <code>psm</code> int 3 Page segmentation mode <code>oem</code> int 3 OCR Engine Mode <code>config_params</code> Dict None Additional Tesseract parameters <code>timeout</code> int 0 OCR timeout in seconds"},{"location":"core-concepts/document-loaders/tesseract/#features","title":"Features","text":"<ul> <li>Text extraction from images</li> <li>Multi-language support</li> <li>Configurable page segmentation</li> <li>Multiple OCR engine modes</li> <li>Custom Tesseract parameters</li> <li>Timeout control</li> <li>Caching support</li> <li>No cloud service required</li> </ul>"},{"location":"core-concepts/document-loaders/tesseract/#notes","title":"Notes","text":"<ul> <li>Vision mode is always enabled</li> <li>Requires Tesseract installation</li> <li>Performance depends on image quality</li> <li>Local processing with no external API calls</li> <li>Language data files must be installed separately</li> </ul>"},{"location":"core-concepts/document-loaders/txt/","title":"Text File Document Loader","text":"<p>The Text File loader is a simple loader for reading plain text files. It has no external dependencies as it uses Python's built-in file handling.</p>"},{"location":"core-concepts/document-loaders/txt/#supported-formats","title":"Supported Formats","text":"<ul> <li>txt</li> </ul>"},{"location":"core-concepts/document-loaders/txt/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/txt/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderTxt\n\n# Initialize the loader with default settings\nloader = DocumentLoaderTxt()\n\n# Load document\npages = loader.load(\"path/to/your/document.txt\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/txt/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderTxt, TxtConfig\n\n# Create configuration\nconfig = TxtConfig(\n    encoding='utf-8',              # Specify text encoding\n    preserve_whitespace=True,      # Preserve original whitespace\n    split_paragraphs=True,         # Split text into paragraphs\n    cache_ttl=600                  # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderTxt(config)\n\n# Load and process document\npages = loader.load(\"path/to/your/document.txt\")\n</code></pre>"},{"location":"core-concepts/document-loaders/txt/#configuration-options","title":"Configuration Options","text":"<p>The <code>TxtConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>encoding</code> str 'utf-8' Text encoding to use <code>preserve_whitespace</code> bool False Whether to preserve whitespace in text <code>split_paragraphs</code> bool False Whether to split text into paragraphs"},{"location":"core-concepts/document-loaders/txt/#features","title":"Features","text":"<ul> <li>Simple text file reading</li> <li>Configurable text encoding</li> <li>Whitespace preservation control</li> <li>Paragraph splitting option</li> <li>Stream-based loading support</li> <li>Caching support</li> <li>No external dependencies required</li> </ul>"},{"location":"core-concepts/document-loaders/txt/#notes","title":"Notes","text":"<ul> <li>Vision mode is not supported for text files</li> <li>BytesIO streams are supported for in-memory processing</li> <li>Default encoding is UTF-8</li> </ul>"},{"location":"core-concepts/document-loaders/web-loader/","title":"Web Document Loader","text":"<p>The Web loader extracts content from web pages using BeautifulSoup. It supports HTML parsing, content cleaning, and custom element handling.</p>"},{"location":"core-concepts/document-loaders/web-loader/#supported-formats","title":"Supported Formats","text":"<ul> <li>html</li> <li>htm</li> <li>xhtml</li> <li>url</li> </ul>"},{"location":"core-concepts/document-loaders/web-loader/#usage","title":"Usage","text":""},{"location":"core-concepts/document-loaders/web-loader/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import DocumentLoaderBeautifulSoup\n\n# Initialize with default settings\nloader = DocumentLoaderBeautifulSoup()\n\n# Load document\npages = loader.load(\"https://example.com\")\n\n# Process extracted content\nfor page in pages:\n    # Access text content\n    text = page[\"content\"]\n</code></pre>"},{"location":"core-concepts/document-loaders/web-loader/#configuration-based-usage","title":"Configuration-based Usage","text":"<pre><code>from extract_thinker import DocumentLoaderBeautifulSoup, BeautifulSoupConfig\n\n# Create configuration\nconfig = BeautifulSoupConfig(\n    header_handling=\"extract\",     # Extract headers as separate content\n    parser=\"lxml\",                # Use lxml parser\n    remove_elements=[             # Elements to remove\n        \"script\", \"style\", \"nav\", \"footer\"\n    ],\n    max_tokens=8192,             # Maximum tokens per page\n    request_timeout=30,          # Request timeout in seconds\n    cache_ttl=600               # Cache results for 10 minutes\n)\n\n# Initialize loader with configuration\nloader = DocumentLoaderBeautifulSoup(config)\n\n# Load and process document\npages = loader.load(\"https://example.com\")\n</code></pre>"},{"location":"core-concepts/document-loaders/web-loader/#configuration-options","title":"Configuration Options","text":"<p>The <code>BeautifulSoupConfig</code> class supports the following options:</p> Option Type Default Description <code>content</code> Any None Initial content to process <code>cache_ttl</code> int 300 Cache time-to-live in seconds <code>header_handling</code> str \"ignore\" How to handle headers <code>parser</code> str \"html.parser\" HTML parser to use <code>remove_elements</code> List[str] None Elements to remove <code>max_tokens</code> int None Maximum tokens per page <code>request_timeout</code> int 10 Request timeout in seconds"},{"location":"core-concepts/document-loaders/web-loader/#features","title":"Features","text":"<ul> <li>Web page content extraction</li> <li>Header handling options</li> <li>Custom element removal</li> <li>Multiple parser support</li> <li>Token limit control</li> <li>Request timeout control</li> <li>Caching support</li> <li>Stream-based loading</li> </ul>"},{"location":"core-concepts/document-loaders/web-loader/#notes","title":"Notes","text":"<ul> <li>Vision mode is not supported</li> <li>Requires internet connection for URLs</li> <li>Local HTML files are supported</li> <li>Respects robots.txt</li> <li>May require custom headers for some sites</li> </ul>"},{"location":"core-concepts/extractors/","title":"Extractor","text":"<p>Extractor is the component that coordinates the extraction from documents. Contains a group of features for document processing like classify. Can be used alone or in group, inside of a Process.</p>"},{"location":"core-concepts/extractors/#basic-extraction","title":"Basic Extraction","text":"<p>The simplest way to extract data is using the <code>Extractor</code> class with a defined contract:</p> <pre><code>from extract_thinker import Extractor, DocumentLoaderPyPdf, Contract\n\nclass InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    total_amount: float\n\n#Initialize the extractor\nextractor = Extractor()\nextractor.load_document_loader(DocumentLoaderPyPdf())\nextractor.load_llm(\"gpt-4o-mini\") # or any other supported model\n\n#Extract data from your document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n\nprint(f\"Invoice #{result.invoice_number}\")\nprint(f\"Date: {result.invoice_date}\")\nprint(f\"Total: ${result.total_amount}\")\n</code></pre>"},{"location":"core-concepts/extractors/#choosing-the-right-model","title":"Choosing the Right Model","text":"<p>When performing extraction, selecting the appropriate model is crucial for balancing performance, accuracy, and cost:</p> <ul> <li>GPT-4o-mini: Best for basic text extraction tasks, similar to OCR. Cost-effective for high-volume processing.</li> <li>GPT-4o: Ideal for tasks requiring deeper understanding of document structure and content.</li> <li>o1 and o1-mini: Perfect for complex extraction requiring reasoning and calculations.</li> </ul>"},{"location":"core-concepts/extractors/#advanced-extraction-with-vision","title":"Advanced Extraction with Vision","text":"<p>For documents that contain images or require visual understanding, you can enable vision capabilities:</p> <pre><code>from extract_thinker import Extractor, Contract\nfrom typing import List\n\nclass ChartData(Contract):\n    title: str\n    data_points: List[float]\n    description: str\n\n#Initialize with vision support\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\n#Extract with vision enabled\nresult = extractor.extract(\n    \"chart.png\",\n    ChartData,\n    vision=True # Enable vision processing\n)\n</code></pre> <p>Note: When using vision capabilities, ensure your documents are high quality images or PDFs for optimal results.</p>"},{"location":"core-concepts/extractors/#adding-context-to-extraction","title":"Adding Context to Extraction","text":"<p>You can provide additional context to help guide the extraction process:</p> <pre><code>from extract_thinker import Extractor, Contract\nclass ResumeContract(Contract):\n    name: str\n    skills: List[str]\n    experience: List[dict]\n\n#Add context about the job requirements\njob_description = {\n    \"role\": \"Software Engineer\",\n    \"required_skills\": [\"Python\", \"AWS\", \"Docker\"]\n}\n\nresult = extractor.extract(\n    \"resume.pdf\",\n    ResumeContract,\n    content=job_description # Add extra context\n)\n</code></pre>"},{"location":"core-concepts/extractors/batch/","title":"Batch Processing with Extractors","text":"<p>ExtractThinker provides powerful batch processing capabilities for handling large volumes of documents efficiently. This feature enables cost-effective processing when immediate response time is not critical.</p>"},{"location":"core-concepts/extractors/batch/#basic-batch-processing","title":"Basic Batch Processing","text":"<p>Here's how to use batch processing with the Extractor:</p> <pre><code>from extract_thinker import Extractor, Contract\n\nclass InvoiceContract(Contract):\n    invoice_number: str\n    total_amount: float\n\n# Initialize batch processing\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o-mini\")\n\n# Create batch job\nbatch_job = extractor.extract_batch(\n    \"invoices/*.pdf\",\n    InvoiceContract\n)\n\n# Monitor status and get results\nstatus = await batch_job.get_status()\nresults = await batch_job.get_result()\n</code></pre>"},{"location":"core-concepts/extractors/batch/#batch-job-status","title":"Batch Job Status","text":"<p>Batch jobs can have the following statuses:</p> <ul> <li><code>queued</code>: Job is waiting to be processed</li> <li><code>processing</code>: Job is currently being processed</li> <li><code>completed</code>: Job has finished successfully</li> <li><code>failed</code>: Job encountered an error</li> </ul>"},{"location":"core-concepts/extractors/image_charts/","title":"Image and Chart Processing","text":"<p>ExtractThinker provides specialized capabilities for processing images and extracting data from charts using vision-enabled models. This guide covers how to effectively use these features.</p>"},{"location":"core-concepts/extractors/image_charts/#basic-vision-processing","title":"Basic Vision Processing","text":"<p>For documents containing images or requiring visual understanding:</p> <pre><code>from extract_thinker import Extractor, Contract\n\nclass InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    lines: List[LineItem]\n\n# Initialize with vision support\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")\n\n# Extract with vision enabled\nresult = extractor.extract(\n    \"invoice.pdf\",\n    InvoiceContract,\n    vision=True  # Enable vision processing\n)\n</code></pre>"},{"location":"core-concepts/extractors/image_charts/#chart-analysis","title":"Chart Analysis","text":"<p>For extracting data from charts and graphs:</p> <pre><code>from extract_thinker import Extractor, Contract\nfrom typing import List, Literal\n\nclass Chart(Contract):\n    classification: Literal['line', 'bar', 'pie']\n    coordinates: List[XYCoordinate]\n    description: str\n\nclass ChartWithContent(Contract):\n    content: str  # Text content from the page\n    chart: Chart  # Extracted chart data\n\n# Initialize extractor for chart analysis\nextractor = Extractor()\nextractor.load_llm(\"gpt-4o\")  # Required for chart analysis\n\n# Extract chart data\nresult = extractor.extract(\n    \"chart.png\",\n    ChartWithContent,\n    vision=True\n)\n</code></pre>"},{"location":"core-concepts/extractors/image_charts/#model-selection-for-visual-tasks","title":"Model Selection for Visual Tasks","text":"<p>Different models are optimized for different visual tasks:</p> <ul> <li>GPT-4o: Required for vision tasks, chart analysis, and complex visual understanding</li> <li>GPT-4o-mini: Not suitable for vision tasks - use for text extraction only</li> </ul>"},{"location":"core-concepts/extractors/image_charts/#best-practices","title":"Best Practices","text":"<ul> <li>Enable vision processing (<code>vision=True</code>) when working with images or charts</li> <li>Use GPT-4o or higher models for vision tasks</li> <li>Consider using a DocumentLoader in combination with vision for optimal results</li> <li>Ensure high-quality input images for best accuracy</li> </ul>"},{"location":"core-concepts/extractors/image_charts/#limitations","title":"Limitations","text":"<ul> <li>Vision processing requires GPT-4o or higher models</li> <li>Processing time may be longer for vision-enabled extraction</li> <li>Image quality significantly impacts extraction accuracy</li> </ul> <p>For more examples and advanced usage, check out the examples directory in the repository. </p>"},{"location":"core-concepts/llm-integration/","title":"LLM Integration","text":"<p>The LLM component in ExtractThinker acts as a bridge between your document processing pipeline and various Language Model providers. It handles request formatting, response parsing, and provider-specific optimizations.</p> Base LLM Implementation <pre><code>import asyncio\nfrom typing import List, Dict, Any, Optional\nimport instructor\nimport litellm\nfrom litellm import Router\nfrom extract_thinker.llm_engine import LLMEngine\nfrom extract_thinker.utils import add_classification_structure, extract_thinking_json\n\n# Add these constants at the top of the file, after the imports\nDYNAMIC_PROMPT_TEMPLATE = \"\"\"Please provide your thinking process within &lt;think&gt; tags, followed by your JSON output.\n\nJSON structure:\n{prompt}\n\nOUTPUT example:\n&lt;think&gt;\nYour step-by-step reasoning and analysis goes here...\n&lt;/think&gt;\n\n##JSON OUTPUT\n{{\n    ...\n}}\n\"\"\"\n\nclass LLM:\n    TIMEOUT = 3000  # Timeout in milliseconds\n    DEFAULT_TEMPERATURE = 0\n\n    def __init__(\n        self,\n        model: str,\n        token_limit: int = None,\n        backend: LLMEngine = LLMEngine.DEFAULT\n    ):\n        \"\"\"Initialize LLM with specified backend.\n\n        Args:\n            model: The model name (e.g. \"gpt-4\", \"claude-3\")\n            token_limit: Optional maximum tokens\n            backend: LLMBackend enum (default: LITELLM)\n        \"\"\"\n        self.model = model\n        self.token_limit = token_limit\n        self.router = None\n        self.is_dynamic = False\n        self.backend = backend\n        self.temperature = self.DEFAULT_TEMPERATURE\n\n        if self.backend == LLMEngine.DEFAULT:\n            self.client = instructor.from_litellm(\n                litellm.completion,\n                mode=instructor.Mode.MD_JSON\n            )\n            self.agent = None\n        elif self.backend == LLMEngine.PYDANTIC_AI:\n            self._check_pydantic_ai()\n            from pydantic_ai import Agent\n            from pydantic_ai.models import KnownModelName\n            from typing import cast\n            import asyncio\n\n            self.client = None\n            self.agent = Agent(\n                cast(KnownModelName, self.model)\n            )\n        else:\n            raise ValueError(f\"Unsupported backend: {self.backend}\")\n\n    @staticmethod\n    def _check_pydantic_ai():\n        \"\"\"Check if pydantic-ai is installed.\"\"\"\n        try:\n            import pydantic_ai\n        except ImportError:\n            raise ImportError(\n                \"Could not import pydantic-ai package. \"\n                \"Please install it with `pip install pydantic-ai`.\"\n            )\n\n    @staticmethod\n    def _get_pydantic_ai():\n        \"\"\"Lazy load pydantic-ai.\"\"\"\n        try:\n            import pydantic_ai\n            return pydantic_ai\n        except ImportError:\n            raise ImportError(\n                \"Could not import pydantic-ai package. \"\n                \"Please install it with `pip install pydantic-ai`.\"\n            )\n\n    def load_router(self, router: Router) -&gt; None:\n        \"\"\"Load a LiteLLM router for model fallbacks.\"\"\"\n        if self.backend != LLMEngine.DEFAULT:\n            raise ValueError(\"Router is only supported with LITELLM backend\")\n        self.router = router\n\n    def set_temperature(self, temperature: float) -&gt; None:\n        \"\"\"Set the temperature for LLM requests.\n\n        Args:\n            temperature (float): Temperature value between 0 and 1\n        \"\"\"\n        self.temperature = temperature\n\n    def set_dynamic(self, is_dynamic: bool) -&gt; None:\n        \"\"\"Set whether the LLM should handle dynamic content.\n\n        When dynamic is True, the LLM will attempt to parse and validate JSON responses.\n        This is useful for handling structured outputs like masking mappings.\n\n        Args:\n            is_dynamic (bool): Whether to enable dynamic content handling\n        \"\"\"\n        self.is_dynamic = is_dynamic\n\n    def request(\n        self,\n        messages: List[Dict[str, str]],\n        response_model: Optional[str] = None\n    ) -&gt; Any:\n        # Handle Pydantic-AI backend differently\n        if self.backend == LLMEngine.PYDANTIC_AI:\n            # Combine messages into a single prompt\n            combined_prompt = \" \".join([m[\"content\"] for m in messages])\n            try:\n                # Create event loop if it doesn't exist\n                try:\n                    loop = asyncio.get_event_loop()\n                except RuntimeError:\n                    loop = asyncio.new_event_loop()\n                    asyncio.set_event_loop(loop)\n\n                result = loop.run_until_complete(\n                    self.agent.run(\n                        combined_prompt, \n                        result_type=response_model if response_model else str\n                    )\n                )\n                return result.data\n            except Exception as e:\n                raise ValueError(f\"Failed to extract from source: {str(e)}\")\n\n        # Uncomment the following lines if you need to calculate max_tokens\n        # contents = map(lambda message: message['content'], messages)\n        # all_contents = ' '.join(contents)\n        # max_tokens = num_tokens_from_string(all_contents)\n\n        # if is sync, response model is None if dynamic true and used for dynamic parsing after llm request\n        request_model = None if self.is_dynamic else response_model\n\n        # Add model structure and prompt engineering if dynamic parsing is enabled\n        working_messages = messages.copy()\n        if self.is_dynamic and response_model:\n            structure = add_classification_structure(response_model)\n            prompt = DYNAMIC_PROMPT_TEMPLATE.format(prompt=structure)\n            working_messages.append({\n                \"role\": \"system\",\n                \"content\": prompt\n            })\n\n        if self.router:\n            response = self.router.completion(\n                model=self.model,\n                messages=working_messages,\n                response_model=request_model,\n                temperature=self.temperature,\n                timeout=self.TIMEOUT,\n            )\n        else:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=working_messages,\n                temperature=self.temperature,\n                response_model=request_model,\n                max_retries=1,\n                max_tokens=self.token_limit,\n                timeout=self.TIMEOUT,\n            )\n\n        # If response_model is provided, return the response directly\n        if self.is_dynamic == False:\n            return response\n\n        # Otherwise get content and handle dynamic parsing if enabled\n        content = response.choices[0].message.content\n        if self.is_dynamic:\n            return extract_thinking_json(content, response_model)\n\n        return content\n\n    def raw_completion(self, messages: List[Dict[str, str]]) -&gt; str:\n        \"\"\"Make raw completion request without response model.\"\"\"\n        if self.router:\n            raw_response = self.router.completion(\n                model=self.model,\n                messages=messages\n            )\n        else:\n            raw_response = litellm.completion(\n                model=self.model,\n                messages=messages,\n                max_tokens=self.token_limit\n            )\n        return raw_response.choices[0].message.content\n\n    def set_timeout(self, timeout_ms: int) -&gt; None:\n        \"\"\"Set the timeout value for LLM requests in milliseconds.\"\"\"\n        self.TIMEOUT = timeout_ms\n</code></pre> <p>The architecture supports two different stacks:</p> <p>Default Stack: Combines instructor and litellm</p> <ul> <li>Uses instructor for structured outputs with Pydantic</li> <li>Leverages litellm for unified model interface</li> </ul> <p>Pydantic AI Stack \ud83e\uddea In Beta</p> <ul> <li>All-in-one solution for Pydantic model integration</li> <li>Handles both model interfacing and structured outputs</li> <li>Built by the Pydantic team (Learn more)</li> </ul>"},{"location":"core-concepts/llm-integration/#backend-options","title":"Backend Options","text":"<pre><code>from extract_thinker import LLM\nfrom extract_thinker.llm_engine import LLMEngine\n\n# Initialize with default stack (instructor + litellm)\nllm = LLM(\"gpt-4o\")\n\n# Or use Pydantic AI stack (Beta)\nllm = LLM(\"openai:gpt-4o\", backend=LLMEngine.PYDANTIC_AI)\n</code></pre> <p>ExtractThinker supports two LLM stacks:</p>"},{"location":"core-concepts/llm-integration/#default-stack-instructor-litellm","title":"Default Stack (instructor + litellm)","text":"<p>The default stack combines instructor for structured outputs and litellm for model interfacing. It leverages LiteLLM's unified API for consistent model access:</p> <pre><code>llm = LLM(\"gpt-4o\", backend=LLMEngine.DEFAULT)\n</code></pre>"},{"location":"core-concepts/llm-integration/#pydantic-ai-stack-beta","title":"Pydantic AI Stack (Beta)","text":"<p>An alternative all-in-one solution for model integration powered by Pydantic AI:</p> <pre><code>llm = LLM(\"openai:gpt-4o\", backend=LLMEngine.PYDANTIC_AI)\n</code></pre> <p>Pydantic AI Limitations</p> <ul> <li>Batch processing is not supported with the Pydantic AI backend</li> <li>Router functionality is not available</li> <li>Requires the <code>pydantic-ai</code> package to be installed</li> </ul> <p>Read more about Pydantic AI features</p>"},{"location":"core-concepts/llm-integration/dynamic-parsing/","title":"Dynamic Parsing","text":"<p>Dynamic parsing enables flexible handling of structured outputs from LLM responses. This feature is particularly useful when reasoning models are used (e.g. DeepSeek R1).</p>"},{"location":"core-concepts/llm-integration/dynamic-parsing/#overview","title":"Overview","text":"<p>The dynamic parsing feature can be enabled using the <code>set_dynamic()</code> method on your LLM instance. When enabled, the LLM will:</p> <ol> <li>Attempt to parse and validate JSON responses</li> <li>Include structured thinking process in the output</li> <li>Handle complex response models dynamically</li> </ol>"},{"location":"core-concepts/llm-integration/dynamic-parsing/#usage","title":"Usage","text":""},{"location":"core-concepts/llm-integration/dynamic-parsing/#heres-how-to-enable-dynamic-parsing","title":"Here's how to enable dynamic parsing:","text":"<pre><code>from extract_thinker import LLM\n\n# Initialize LLM\nllm = LLM(\"ollama/deepseek-r1:1.5b\")\n\n# Enable dynamic parsing\nllm.set_dynamic(True)\n</code></pre>"},{"location":"core-concepts/llm-integration/dynamic-parsing/#uses-this-template-structure","title":"Uses this template structure:","text":"<pre><code>Please provide your thinking process within &lt;think&gt; tags, followed by your JSON output.\n\nJSON structure:\n{your_structure}\n\nOUTPUT example:\n&lt;think&gt;\nYour step-by-step reasoning and analysis goes here...\n&lt;/think&gt;\n\n##JSON OUTPUT\n{\n    ...\n}\n</code></pre>"},{"location":"core-concepts/llm-integration/dynamic-parsing/#example-invoice-extraction","title":"Example: Invoice Extraction","text":"<p>Here's a complete example of using dynamic parsing for invoice extraction:</p> <pre><code>from extract_thinker import LLM, Extractor\nfrom extract_thinker.document_loader import DocumentLoaderPyPdf\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n# Define your invoice model\nclass InvoiceData(BaseModel):\n    invoice_number: str\n    date: str\n    total_amount: float\n    vendor_name: str\n    line_items: List[dict]\n    payment_terms: Optional[str]\n\n# Initialize LLM with dynamic parsing\nllm = LLM(\"ollama/deepseek-r1:1.5b\")\nllm.set_dynamic(True)  # Enable dynamic JSON parsing\n\n# Setup document loader and extractor\ndocument_loader = DocumentLoaderPyPdf()\nextractor = Extractor(document_loader=document_loader, llm=llm)\n\n# Extract information from invoice\nresult = extractor.extract(\"path/to/invoice.pdf\", response_model=InvoiceData)\n</code></pre>"},{"location":"core-concepts/process/","title":"Process","text":"<p>Process is a component that orchestrates the document processing workflow, allowing you to combine multiple DocumentLoaders and Extractors for complex document processing tasks.</p> <p>The workflow consists of: <pre><code>split_content = process.load_file(path)\\\n    .split(classifications)\\\n    .extract()\n</code></pre></p> <p>This creates a pipeline that:</p> <ol> <li> <p>Loads the document</p> </li> <li> <p>Splits it into logical sections</p> </li> <li> <p>Extracts structured data from each section</p> </li> </ol> Process Implementation <pre><code>import asyncio\nfrom typing import IO, Any, Dict, List, Optional, Union\nfrom extract_thinker.image_splitter import ImageSplitter\nfrom extract_thinker.models.classification_response import ClassificationResponse\nfrom extract_thinker.models.classification_strategy import ClassificationStrategy\nfrom extract_thinker.models.completion_strategy import CompletionStrategy\nfrom extract_thinker.models.splitting_strategy import SplittingStrategy\nfrom extract_thinker.extractor import Extractor\nfrom extract_thinker.models.classification import Classification\nfrom extract_thinker.document_loader.document_loader import DocumentLoader\nfrom extract_thinker.models.classification_tree import ClassificationTree\nfrom extract_thinker.splitter import Splitter\nfrom extract_thinker.models.doc_groups import (\n    DocGroups,\n)\nfrom extract_thinker.utils import get_image_type\n\nclass Process:\n    def __init__(self):\n        # self.extractors: List[Extractor] = []\n        self.doc_groups: Optional[DocGroups] = None\n        self.split_classifications: List[Classification] = []\n        self.extractor_groups: List[List[Extractor]] = []  # for classication\n        self.document_loaders_by_file_type: Dict[str, DocumentLoader] = {}\n        self.document_loader: Optional[DocumentLoader] = None\n        self.file_path: Optional[str] = None\n        self.file_stream: Optional[IO] = None\n        self.splitter: Optional[Splitter] = None\n        self._content_loaded: bool = False  # New internal flag\n\n    def set_document_loader_for_file_type(self, file_type: str, document_loader: DocumentLoader):\n        if self.document_loader is not None:\n            raise ValueError(\"Cannot set a document loader for a specific file type when a default loader is already set.\")\n        self.document_loaders_by_file_type[file_type] = document_loader\n\n    def load_document_loader(self, document_loader: DocumentLoader):\n        if self.document_loaders_by_file_type:\n            raise ValueError(\"Cannot set a default document loader when specific loaders are already set.\")\n        self.document_loader = document_loader\n        return self\n\n    def load_splitter(self, splitter: Splitter):\n        \"\"\"\n        Load a splitter and configure vision mode if needed.\n\n        Args:\n            splitter: The splitter instance to use\n        Returns:\n            self for method chaining\n        \"\"\"\n        self.splitter = splitter\n\n        # Check if the splitter is an ImageSplitter\n        is_vision = isinstance(splitter, ImageSplitter)\n\n        # Configure vision mode for any loaded document loaders\n        if self.document_loader:\n            self.document_loader.set_vision_mode(is_vision)\n\n        for loader in self.document_loaders_by_file_type.values():\n            loader.set_vision_mode(is_vision)\n\n        return self\n\n    def add_classify_extractor(self, extractor_groups: List[List[Extractor]]):\n        for extractors in extractor_groups:\n            self.extractor_groups.append(extractors)\n        return self\n\n    async def _classify_async(self, extractor: Extractor, file: str, classifications: List[Classification], image: bool = False):\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(None, extractor.classify, file, classifications, image)\n\n    def classify(self, file: str, classifications, strategy: ClassificationStrategy = ClassificationStrategy.CONSENSUS, threshold: int = 9, image: bool = False) -&gt; Optional[Classification]:\n        if not isinstance(threshold, int) or threshold &lt; 1 or threshold &gt; 10:\n            raise ValueError(\"Threshold must be an integer between 1 and 10\")\n\n        result = asyncio.run(self.classify_async(file, classifications, strategy, threshold, image))\n        return result\n\n    async def classify_async(\n        self,\n        file: str,\n        classifications: Union[List[Classification], ClassificationTree],\n        strategy: ClassificationStrategy = ClassificationStrategy.CONSENSUS,\n        threshold: int = 9,\n        image: str = False\n    ) -&gt; Optional[Classification]:\n        if not isinstance(threshold, int) or threshold &lt; 1 or threshold &gt; 10:\n            raise ValueError(\"Threshold must be an integer between 1 and 10\")\n\n        if isinstance(classifications, ClassificationTree):\n            return await self._classify_tree_async(file, classifications, threshold, image)\n\n        # Try each layer of extractors until we get a valid result\n        for extractor_group in self.extractor_groups:\n            group_classifications = await asyncio.gather(*(\n                self._classify_async(extractor, file, classifications, image) \n                for extractor in extractor_group\n            ))\n\n            try:\n                # Attempt to get result based on strategy\n                if strategy == ClassificationStrategy.CONSENSUS:\n                    if len(set(c.name for c in group_classifications)) == 1:\n                        return group_classifications[0]\n\n                elif strategy == ClassificationStrategy.HIGHER_ORDER:\n                    return max(group_classifications, key=lambda c: c.confidence)\n\n                elif strategy == ClassificationStrategy.CONSENSUS_WITH_THRESHOLD:\n                    if len(set(c.name for c in group_classifications)) == 1:\n                        if all(c.confidence &gt;= threshold for c in group_classifications):\n                            return group_classifications[0]\n\n                # If we get here, current layer didn't meet criteria - continue to next layer\n                continue\n\n            except Exception as e:\n                # If there's an error processing this layer, try the next one\n                print(f\"Layer failed with error: {str(e)}\")\n                continue\n\n        # If we've tried all layers and none worked\n        raise ValueError(\"No consensus could be reached on the classification of the document across any layer. Please try again with a different strategy or threshold.\")\n\n    async def _classify_tree_async(\n        self, \n        file: str, \n        classification_tree: ClassificationTree, \n        threshold: float,\n        image: bool\n    ) -&gt; Optional[ClassificationResponse]:\n        if not isinstance(threshold, (int, float)) or threshold &lt; 1 or threshold &gt; 10:\n            raise ValueError(\"Threshold must be a number between 1 and 10\")\n\n        \"\"\"\n        Perform classification in a hierarchical, level-by-level approach.\n        \"\"\"\n        best_classification = None\n        current_nodes = classification_tree.nodes\n\n        while current_nodes:\n            # Get the list of classifications at the current level\n            classifications = [node.classification for node in current_nodes]\n\n            # Classify among the current level's classifications\n            classification_response = await self._classify_async(\n                extractor=self.extractor_groups[0][0],\n                file=file, \n                classifications=classifications, \n                image=image\n            )\n\n            if classification_response.confidence &lt; threshold:\n                raise ValueError(\n                    f\"Classification confidence {classification_response.confidence} \"\n                    f\"for '{classification_response.name}' is below the threshold of {threshold}.\"\n                )\n\n            best_classification = classification_response\n\n            matching_node = next(\n                (\n                    node for node in current_nodes\n                    if node.classification.name == best_classification.name\n                ),\n                None\n            )\n\n            if matching_node is None:\n                raise ValueError(\n                    f\"No matching node found for classification '{classification_response.name}'.\"\n                )\n\n            if matching_node.children:\n                current_nodes = matching_node.children\n            else:\n                break\n\n        return best_classification if best_classification else None\n\n    async def classify_extractor(self, session, extractor, file):\n        return await session.run(extractor.classify, file)\n\n    # check if there is only the default one, if not, get from the file type. if none is present, raise an error\n    def get_document_loader(self, file):\n        if self.document_loader is not None:\n            return self.document_loader\n\n        filetype = get_image_type(file)\n        return self.document_loaders_by_file_type.get(filetype, None)\n\n    def load_file(self, file):\n        self.file_path = file\n        return self\n\n    def split(self, classifications: List[Classification], strategy: SplittingStrategy = SplittingStrategy.EAGER):\n        \"\"\"Split the document into groups based on classifications.\"\"\"\n        if self.splitter is None:\n            raise ValueError(\"No splitter loaded. Please load a splitter using load_splitter() before splitting.\")\n\n        self.split_classifications = classifications\n\n        document_loader = self.get_document_loader(self.file_path)\n        if document_loader is None:\n            raise ValueError(\"No suitable document loader found for file type\")\n\n        # Load content using the new unified load() method\n        if self.file_path:\n            pages = document_loader.load(self.file_path)\n        elif self.file_stream:\n            pages = document_loader.load(self.file_stream)\n        else:\n            raise ValueError(\"No file or stream available\")\n\n        if len(pages) &lt; 2:\n            raise ValueError(\"Document must have at least 2 pages\")\n\n        # Process based on strategy\n        if strategy == SplittingStrategy.EAGER:\n            eager_group = self.splitter.split_eager_doc_group(pages, classifications)\n            self.doc_groups = eager_group\n        else:  # LAZY strategy\n            if document_loader.can_handle_paginate(self.file_path):\n                processed_groups = self.splitter.split_lazy_doc_group(pages, classifications)\n                self.doc_groups = processed_groups.doc_groups\n            else:\n                raise ValueError(\"Document Type does not support lazy splitting. for now only pdf is supported\")\n\n        return self\n\n    def extract(self, \n                vision: bool = False,\n                completion_strategy: Optional[CompletionStrategy] = CompletionStrategy.FORBIDDEN) -&gt; List[Any]:\n        \"\"\"Extract information from the document groups.\"\"\"\n        if self.doc_groups is None:\n            raise ValueError(\"Document groups have not been initialized\")\n\n        async def _extract(doc_group):\n            # Find matching classification and extractor\n            classificationStr = doc_group.classification\n            extractor: Optional[Extractor] = None\n            contract = None\n\n            for classification in self.split_classifications:\n                if classification.name == classificationStr:\n                    extractor = classification.extractor\n                    # If an extraction_contract is provided, use it; otherwise, use the default contract\n                    contract = classification.extraction_contract or classification.contract\n                    break\n\n            if extractor is None:\n                raise ValueError(\"Extractor not found for classification\")\n\n            # Get document loader\n            document_loader = self.get_document_loader(self.file_path)\n            if document_loader is None:\n                raise ValueError(\"No suitable document loader found for file type\")\n\n            # Load content using the new unified load() method\n            if self.file_path:\n                pages = document_loader.load(self.file_path)\n            elif self.file_stream:\n                pages = document_loader.load(self.file_stream)\n            else:\n                raise ValueError(\"No file or stream available\")\n\n            # Get pages for this group\n            group_pages = [pages[i - 1] for i in doc_group.pages]\n\n            # Set flag to skip loading since content is already processed\n            extractor.set_skip_loading(True)\n            try:\n                result = await extractor.extract_async(\n                    source=group_pages,\n                    response_model=contract,\n                    vision=vision,\n                    content=None,\n                    completion_strategy=completion_strategy\n                )\n            finally:\n                # Reset flag after extraction\n                extractor.set_skip_loading(False)\n\n            return result\n\n        async def process_doc_groups(groups: List[Any]) -&gt; List[Any]:\n            tasks = [_extract(group) for group in groups]\n            try:\n                processedGroups = await asyncio.gather(*tasks)\n                return processedGroups\n            except Exception as e:\n                print(f\"An error occurred: {e}\")\n                raise\n\n        loop = asyncio.get_event_loop()\n        processedGroups = loop.run_until_complete(\n            process_doc_groups(self.doc_groups)\n        )\n\n        return processedGroups\n</code></pre>"},{"location":"core-concepts/process/#using-multiple-documentloaders","title":"Using Multiple DocumentLoaders","text":"<p>You can configure different DocumentLoaders for specific file types:</p> <pre><code>from extract_thinker import Process\nfrom extract_thinker.document_loader import (\n    DocumentLoaderTesseract,\n    DocumentLoaderPyPdf,\n    DocumentLoaderAzureForm\n)\n\nprocess = Process()\n\n# Set loaders for specific file types\nprocess.set_document_loader_for_file_type(\n    \"pdf\", DocumentLoaderPyPdf()\n)\nprocess.set_document_loader_for_file_type(\n    \"png\", DocumentLoaderTesseract(tesseract_path)\n)\n\n# Or set a default loader\nprocess.load_document_loader(\n    DocumentLoaderAzureForm(subscription_key, endpoint)\n)\n</code></pre>"},{"location":"core-concepts/process/#using-multiple-extractors","title":"Using Multiple Extractors","text":"<p>You can use multiple extractors for different document types or processing stages:</p> <pre><code>from extract_thinker import Extractor, Classification\n\n# Initialize extractors with different models\ngpt4_extractor = Extractor(document_loader)\ngpt4_extractor.load_llm(\"gpt-4o\")\n\nclaude_extractor = Extractor(document_loader)\nclaude_extractor.load_llm(\"claude-3-haiku-20240307\")\n\n# Create classifications with specific extractors\nclassifications = [\n    Classification(\n        name=\"Invoice\",\n        description=\"This is an invoice\",\n        contract=InvoiceContract,\n        extractor=gpt4_extractor\n    ),\n    Classification(\n        name=\"License\",\n        description=\"This is a license\",\n        contract=LicenseContract,\n        extractor=claude_extractor\n    )\n]\n\n# Process will use the appropriate extractor for each document type\nresult = process.load_file(\"document.pdf\")\\\n    .split(classifications)\\\n    .extract()\n</code></pre>"},{"location":"core-concepts/splitters/","title":"Splitters","text":"<p>In document processing, splitting enables the separation of individual documents or sections within a combined file. This task is especially crucial when handling batches of documents where different parts may need distinct processing, and always with Sonnet. This can be done with two strategies: Eager and Lazy.</p>"},{"location":"core-concepts/splitters/#page-level-processing","title":"Page-Level Processing","text":"<p>Splitters work at the page level, determining which pages belong together as a single document. For example:</p> <ul> <li> <p>A 10-page PDF might contain three separate invoices</p> </li> <li> <p>A scanned document might contain multiple forms</p> </li> <li> <p>A batch of documents might need to be separated by document type</p> </li> </ul> <p>The challenge is determining where one document ends and another begins, which is where our splitting strategies come in.</p>"},{"location":"core-concepts/splitters/#eager-vs-lazy-approaches","title":"Eager vs. Lazy Approaches","text":"<p>Eager and Lazy splitting have distinct use cases based on document size and the complexity of relationships between pages.</p>"},{"location":"core-concepts/splitters/#eager-splitting","title":"Eager Splitting","text":"<p>Eager splitting processes all pages in a single pass, identifying and dividing all sections at once. It's efficient for small to medium-sized documents where context size does not limit performance.</p> <pre><code>from extract_thinker import Splitter, SplittingStrategy\n\nsplitter = Splitter()\nresult = splitter.split(\n    document,\n    strategy=SplittingStrategy.EAGER\n)\n</code></pre> <p>Benefits of Eager Splitting: - Speed: Faster processing since all split points are determined upfront - Simplicity: Ideal for documents that fit entirely within the model's context window - Consistency: Better for documents where relationships between pages are important</p>"},{"location":"core-concepts/splitters/#lazy-splitting","title":"Lazy Splitting","text":"<p>Lazy splitting processes pages incrementally in chunks, assessing smaller groups of pages at a time to decide if they belong together. In this use case, groups of two pages are processed and checked for continuity, allowing it to scale efficiently for larger documents.</p> <pre><code>result = splitter.split(\n    document,\n    strategy=SplittingStrategy.LAZY\n)\n</code></pre> <p>Benefits of Lazy Splitting: - Scalability: Well-suited for documents that exceed the model's context window - Memory Efficiency: Processes only what's needed when needed - Flexibility: Better for streaming or real-time processing</p> Base Splitter Implementation <p>The base Splitter class provides both eager and lazy implementations: <pre><code>import asyncio\nfrom typing import Any, List\nfrom abc import ABC, abstractmethod\n\nfrom extract_thinker.models.classification import Classification\nfrom extract_thinker.models.doc_group import DocGroups, DocGroup\nfrom extract_thinker.models.doc_groups2 import DocGroups2\nfrom extract_thinker.models.eager_doc_group import EagerDocGroup\n\n\nclass Splitter(ABC):\n    @abstractmethod\n    def belongs_to_same_document(self, page1: Any, page2: Any, contract: str) -&gt; DocGroups2:\n        pass\n\n    @abstractmethod\n    def split_lazy_doc_group(self, lazy_doc_group: List[Any], classifications: List[Classification]) -&gt; DocGroups:\n        pass\n\n    @abstractmethod\n    def split_eager_doc_group(self, lazy_doc_group: List[Any], classifications: List[Classification]) -&gt; DocGroups:\n        pass\n\n    def split_document_into_groups(self, document: List[Any]) -&gt; List[List[Any]]:\n        page_per_split = 2\n        split = []\n        if len(document) == 1:\n            return [document]\n        for i in range(0, len(document) - 1):\n            group = document[i: i + page_per_split]\n            split.append(group)\n        return split\n\n    async def process_split_groups(self, split: List[List[Any]], contract: str) -&gt; List[DocGroups2]:\n        # Create asynchronous tasks for processing each group\n        tasks = [self.process_group(x, contract) for x in split]\n        try:\n            # Execute all tasks concurrently and wait for all to complete\n            doc_groups = await asyncio.gather(*tasks)\n            return doc_groups\n        except Exception as e:\n            # Handle possible exceptions that might occur during task execution\n            print(f\"An error occurred: {e}\")\n            raise\n\n    async def process_group(self, group: List[Any], contract: str) -&gt; DocGroups2:\n        page2 = group[1] if len(group) &gt; 1 else None\n        return self.belongs_to_same_document(group[0], page2, contract)\n\n    def aggregate_doc_groups(self, doc_groups_tasks: List[DocGroups2]) -&gt; DocGroups:\n        \"\"\"\n        Aggregate the results from belongs_to_same_document comparisons into final document groups.\n        This is the base implementation that can be used by all splitter implementations.\n        \"\"\"\n        doc_groups = DocGroups()\n        current_group = DocGroup(pages=[], classification=\"\")\n        page_number = 1\n\n        if not doc_groups_tasks:\n            return doc_groups\n\n        # Handle the first group\n        doc_group = doc_groups_tasks[0]\n        if doc_group.belongs_to_same_document:\n            current_group.pages = [1, 2]\n            current_group.classification = doc_group.classification_page1\n        else:\n            # First page is its own document\n            current_group.pages = [1]\n            current_group.classification = doc_group.classification_page1\n            doc_groups.doc_groups.append(current_group)\n\n            # Start new group with second page\n            current_group = DocGroup(pages=[2], classification=doc_group.classification_page2)\n\n        page_number += 1\n\n        # Process remaining groups\n        for doc_group in doc_groups_tasks[1:]:\n            if doc_group.belongs_to_same_document:\n                current_group.pages.append(page_number + 1)\n            else:\n                doc_groups.doc_groups.append(current_group)\n                current_group = DocGroup(\n                    pages=[page_number + 1],\n                    classification=doc_group.classification_page2\n                )\n            page_number += 1\n\n        # Add the last group\n        doc_groups.doc_groups.append(current_group)\n\n        return doc_groups\n</code></pre></p>"},{"location":"core-concepts/splitters/#available-splitters","title":"Available Splitters","text":"<p>ExtractThinker provides two main splitter implementations:</p> <ul> <li>Text Splitter: For text-based document splitting</li> <li>Image Splitter: For image-based document splitting</li> </ul>"},{"location":"core-concepts/splitters/#recommended-approach","title":"Recommended Approach","text":"<p>For most IDP use cases, Eager Splitting is appropriate since it offers: - Simpler implementation - Better handling of page relationships - Faster processing for typical document sizes (under 50 pages)</p> <p>However, consider Lazy Splitting when: - Processing very large documents (50+ pages) - Working with limited memory - Handling streaming document inputs</p>"},{"location":"core-concepts/splitters/#best-practices","title":"Best Practices","text":"<ul> <li>Choose strategy based on document size and page count</li> <li>Consider context window limitations of your LLM</li> </ul>"},{"location":"core-concepts/splitters/image/","title":"Image Splitter","text":"<p>Image Splitter is specialized for handling image-based document splitting by analyzing visual consistency and layout patterns between pages.</p>"},{"location":"core-concepts/splitters/image/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import ImageSplitter, Process, SplittingStrategy\nfrom extract_thinker.document_loader import DocumentLoaderTesseract\n\n# Initialize process and loader\nprocess = Process()\nprocess.load_document_loader(DocumentLoaderTesseract(tesseract_path))\n\n# Initialize image splitter with vision model\nprocess.load_splitter(ImageSplitter(\"claude-3-5-sonnet-20241022\"))\n\n# Split document\nresult = process.load_file(\"document.pdf\")\\\n    .split(classifications, strategy=SplittingStrategy.EAGER)\\\n    .extract()\n</code></pre>"},{"location":"core-concepts/splitters/text/","title":"Text Splitter","text":"<p>Text Splitter is designed to handle text-based document splitting by analyzing content continuity and relationships between pages.</p>"},{"location":"core-concepts/splitters/text/#basic-usage","title":"Basic Usage","text":"<pre><code>from extract_thinker import TextSplitter, Process, SplittingStrategy\nfrom extract_thinker.document_loader import DocumentLoaderTesseract\n\n# Initialize process and loader\nprocess = Process()\nprocess.load_document_loader(DocumentLoaderTesseract(tesseract_path))\n\n# Initialize text splitter with model\nprocess.load_splitter(TextSplitter(\"claude-3-5-sonnet-20241022\"))\n\n# Split document\nresult = process.load_file(\"document.pdf\")\\\n    .split(classifications, strategy=SplittingStrategy.EAGER)\\\n    .extract()\n</code></pre>"},{"location":"examples/aws-stack/","title":"AWS Textract with Claude Example","text":"<p>This guide demonstrates how to use AWS Textract combined with Claude for powerful document processing.</p>"},{"location":"examples/aws-stack/#basic-setup","title":"Basic Setup","text":"<p>Here's how to combine AWS Textract's OCR capabilities with Claude:</p> <pre><code>from extract_thinker import Extractor, Contract, LLM, DocumentLoaderTextract\nfrom typing import List\nfrom pydantic import Field\n\nclass InvoiceContract(Contract):\n    invoice_number: str = Field(\"Invoice number\")\n    invoice_date: str = Field(\"Invoice date\")\n    total_amount: float = Field(\"Total amount\")\n    lines: List[LineItem] = Field(\"List of line items\")\n\n# Initialize AWS Textract\nextractor = Extractor()\nextractor.load_document_loader(\n    DocumentLoaderTextract(\n        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n        region_name=os.getenv(\"AWS_REGION\")\n    )\n)\n\n# Configure Claude\nllm = LLM(\n    \"anthropic/claude-3-haiku-20240307\",\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n)\nextractor.load_llm(llm)\n\n# Process document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n</code></pre>"},{"location":"examples/aws-stack/#cost-optimization","title":"Cost Optimization","text":"<p>AWS Textract pricing:</p> <ul> <li> <p>Basic text detection: $1.50 per 1,000 pages (first 1M pages/month)                        $0.60 per 1,000 pages (over 1M pages/month)</p> </li> <li> <p>Tables: $15.00 per 1,000 pages (first 1M pages/month)          $10.00 per 1,000 pages (over 1M pages/month)</p> </li> </ul> <p>Combined with Claude (via AWS Bedrock):</p> <ul> <li> <p>Claude 3 Haiku: $0.00025 per 1K input tokens, $0.00125 per 1K output tokens</p> </li> <li> <p>Claude 3 Sonnet: $0.003 per 1K input tokens, $0.015 per 1K output tokens</p> </li> </ul> <p>Approximate cost per page (first 1M pages/month): - Basic text only: $0.0015 per page ($1.50/1000) - With tables: $0.0165 per page ($1.50/1000 + $15.00/1000) - Plus Claude costs (varies by token length and model choice)</p>"},{"location":"examples/azure-stack/","title":"Azure Document Intelligence Example","text":"<p>This guide demonstrates how to use Azure Document Intelligence with Phi-3 models for efficient document processing.</p>"},{"location":"examples/azure-stack/#basic-setup","title":"Basic Setup","text":"<p>Here's a complete example using Azure Document Intelligence with Phi-3:</p> <pre><code>from extract_thinker import Extractor, Contract, LLM, DocumentLoaderAzureForm\nfrom typing import List\nfrom pydantic import Field\n\nclass InvoiceContract(Contract):\n    invoice_number: str = Field(\"Invoice number\")\n    invoice_date: str = Field(\"Invoice date\")\n    total_amount: float = Field(\"Total amount\")\n    lines: List[LineItem] = Field(\"List of line items\")\n\n# Initialize Azure Document Intelligence\nsubscription_key = os.getenv(\"AZURE_SUBSCRIPTION_KEY\")\nendpoint = os.getenv(\"AZURE_ENDPOINT\")\napi_key = os.getenv(\"AZURE_AI_API_KEY\")\n\nextractor = Extractor()\nextractor.load_document_loader(\n    DocumentLoaderAzureForm(subscription_key, endpoint)\n)\n\n# Configure environment variables for Azure\nimport os\nos.environ[\"AZURE_API_KEY\"] = api_key\nos.environ[\"AZURE_API_BASE\"] = \"https://your-endpoint.inference.ai.azure.com\"\nos.environ[\"AZURE_API_VERSION\"] = \"v1\"\n\n# Configure Phi-3 mini model\nextractor.load_llm(\"azure/Phi-3-mini-128k-instruct\")\n\n# Process document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n</code></pre>"},{"location":"examples/azure-stack/#cost-optimization","title":"Cost Optimization","text":"<p>Azure Document Intelligence offers different pricing tiers:</p> <ul> <li>Read: Basic OCR functionality ($0.001 per page)</li> <li>Prebuilt Layout: Structure detection ($0.01 per page)</li> <li>Custom Layout: Higher cost (discouraged)</li> </ul>"},{"location":"examples/azure-stack/#best-practices","title":"Best Practices","text":"<p>Document Type Selection</p> <ul> <li>Use \"prebuilt-layout\" when vision is not available</li> <li>Use \"read\" for most of the documents</li> <li>Consider custom layout only for specific needs, like signatures</li> </ul>"},{"location":"examples/google-stack/","title":"Google Document AI Example","text":"<p>This guide shows how to use Google Document AI for advanced document processing with ExtractThinker, including integration with Gemini models for enhanced extraction capabilities.</p>"},{"location":"examples/google-stack/#overview","title":"Overview","text":"<p>Google Document AI is a powerful solution that provides:</p> <ul> <li>OCR and structural parsing</li> <li>Classification capabilities</li> <li>Specialized domain extractors (invoices, W2 forms, bank statements, etc.)</li> <li>Layout parsing and form processing</li> </ul>"},{"location":"examples/google-stack/#basic-setup","title":"Basic Setup","text":"<p>First, install the required dependencies:</p> <pre><code>pip install extract-thinker google-cloud-documentai\n</code></pre> <p>Here's how to use Google Document AI with ExtractThinker:</p> <pre><code>from extract_thinker import Extractor, Contract\nfrom extract_thinker.document_loader import DocumentLoaderGoogleDocumentAI\nfrom typing import List\nfrom pydantic import Field\n\nclass InvoiceLineItem(Contract):\n    description: str = Field(description=\"Description of the item\")\n    quantity: int = Field(description=\"Quantity of items purchased\")\n    unit_price: float = Field(description=\"Price per unit\")\n    amount: float = Field(description=\"Total amount for this line\")\n\nclass InvoiceContract(Contract):\n    invoice_number: str = Field(description=\"Unique invoice identifier\")\n    invoice_date: str = Field(description=\"Date of the invoice\")\n    total_amount: float = Field(description=\"Overall total amount\")\n    line_items: List[InvoiceLineItem] = Field(description=\"List of items in this invoice\")\n\n# Initialize Google Document AI\nextractor = Extractor()\nextractor.load_document_loader(\n    DocumentLoaderGoogleDocumentAI(\n        project_id=os.getenv(\"DOCUMENTAI_PROJECT_ID\"),\n        location=os.getenv(\"DOCUMENTAI_LOCATION\"),  # 'us' or 'eu'\n        processor_id=os.getenv(\"DOCUMENTAI_PROCESSOR_ID\"),\n        credentials=os.getenv(\"DOCUMENTAI_GOOGLE_CREDENTIALS\")\n    )\n)\n\n# Configure Gemini model (recommended for enhanced extraction)\nextractor.load_llm(\"vertex_ai/gemini-2.0-flash-exp\")\n\n# Process document\nresult = extractor.extract(\n    source=\"invoice.pdf\",\n    response_model=InvoiceContract,\n    vision=True  # Enable vision mode for better results with Gemini\n)\n</code></pre>"},{"location":"examples/google-stack/#document-splitting","title":"Document Splitting","text":"<p>ExtractThinker provides powerful document splitting capabilities that can be used with Google Document AI. Here's how to implement document splitting:</p> <pre><code>from extract_thinker.process import Process\nfrom extract_thinker.splitter import SplittingStrategy\nfrom extract_thinker.image_splitter import ImageSplitter\n\n# Create a Process instance\nprocess = Process()\n\n# Configure the splitter with Gemini model\nimage_splitter = ImageSplitter(model=\"vertex_ai/gemini-2.0-flash-exp\")\nprocess.load_splitter(image_splitter)\n\n# Define your classifications (e.g., Invoice, Driver License)\nmy_classifications = [invoice_class, driver_license_class]\n\n# Process a combined document with EAGER strategy\nBULK_DOC_PATH = \"path/to/combined_documents.pdf\"\n\nresult = (process.load_file(BULK_DOC_PATH)\n    .split(my_classifications, strategy=SplittingStrategy.EAGER)\n    .extract(vision=True))\n\n# Process results\nfor doc_content in result:\n    print(f\"Document Type: {type(doc_content).__name__}\")\n    print(doc_content.json(indent=2))\n</code></pre> <p>More information about document splitting can be found in the document splitting section.</p> <p>Document OCR: Basic text extraction and layout analysis</p> <ul> <li>Best paired with vision-enabled models like Gemini</li> <li>Most cost-effective for basic OCR needs</li> </ul> <p>Layout Parser: Advanced structural analysis</p> <ul> <li>Use when vision capabilities aren't available</li> <li>Provides detailed document structure information</li> </ul> <p>Specialized Processors: Domain-specific extraction</p> <ul> <li>Invoice Parser</li> <li>Form Parser</li> <li>US Driver License Parser</li> <li>And more...</li> </ul>"},{"location":"examples/google-stack/#cost-optimization","title":"Cost Optimization","text":""},{"location":"examples/google-stack/#document-ai-pricing-as-of-2024","title":"Document AI Pricing (as of 2024)","text":"<p>Document OCR: $1.50 per 1,000 pages</p> <ul> <li>Volume discounts after 5M pages/month</li> <li>Most cost-effective for basic OCR needs</li> </ul> <p>Layout Parser: $10 per 1,000 pages</p> <ul> <li>Good for structural analysis without vision models</li> </ul> <p>Form Parser and Custom Extractors: $30 per 1,000 pages</p> <ul> <li>Volume discounts after 1M pages/month</li> <li>Best for complex form processing</li> </ul> <p>Specialized Processors: Varies by type</p> <ul> <li>Example: Invoice parsing at $0.10 per 10 pages</li> <li>Includes pre-trained field extraction</li> </ul>"},{"location":"examples/google-stack/#cost-effective-strategies","title":"Cost-Effective Strategies","text":"<p>Basic OCR + Gemini:</p> <ul> <li>Use Document OCR ($0.0015/page)</li> <li>Combine with Gemini 2.0 Flash (~$0.0002/page)</li> <li>Total: ~$0.0017/page</li> </ul> <p>Layout Parser + LLM:</p> <ul> <li>Use Layout Parser ($0.01/page)</li> <li>Add LLM processing (~$0.0002/page)</li> <li>Total: ~$0.0102/page</li> </ul> <p>Pure LLM Approach:</p> <ul> <li>Use Gemini's vision capabilities directly</li> <li>Cost: ~$0.0002/page</li> <li>Note: May have lower accuracy for complex documents</li> </ul>"},{"location":"examples/google-stack/#supported-formats","title":"Supported Formats","text":"<ul> <li><code>PDF</code> (up to 2000 pages or 20MB)</li> <li>Images: <code>JPEG</code>, <code>PNG</code>, <code>TIFF</code>, <code>GIF</code></li> <li>Office formats: <code>DOCX</code>, <code>XLSX</code>, <code>PPTX</code></li> <li>Web: <code>HTML</code></li> </ul> <p>For more examples and implementation details, check out the ExtractThinker repository or the related article on Medium.</p>"},{"location":"examples/groq-processing/","title":"Processing Documents with Groq","text":"<p>\u26a0\ufe0f Warning: Vision-based processing may not be available with Groq models. For image or document processing that requires vision capabilities, consider using other providers like Google Document AI, Azure Document Intelligence, or AWS Textract.</p> <p>This guide demonstrates how to process documents using Groq's powerful LLMs.</p>"},{"location":"examples/groq-processing/#basic-setup","title":"Basic Setup","text":"<p>Here's a basic example of document extraction using Groq:</p> <pre><code>from extract_thinker import Extractor\nfrom extract_thinker.document_loader.document_loader_pypdf import DocumentLoaderPyPdf\nfrom typing import List\nfrom pydantic import Field\n\nclass InvoiceContract(Contract):\n    lines: List[LineItem] = Field(\"List of line items in the invoice\")\n\n# Initialize extractor with PyPDF loader\nextractor = Extractor()\nextractor.load_document_loader(DocumentLoaderPyPdf())\n\n# Configure Groq\nextractor.load_llm(\"groq/llama-3.2-11b-vision-preview\")\n\n# Process document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n</code></pre>"},{"location":"examples/groq-processing/#classification-example","title":"Classification Example","text":"<p>You can also use Groq for document classification:</p> <pre><code>from extract_thinker import Process\nfrom extract_thinker.models.classification import Classification\n\n# Setup process\nprocess = Process()\nprocess.add_classify_extractor([[extractor]])\n\n# Define classifications\nclassifications = [\n    Classification(\n        name=\"Invoice\",\n        description=\"This is an invoice document\", \n        contract=InvoiceContract\n    ),\n    Classification(\n        name=\"Driver License\",\n        description=\"This is a driver license document\",\n        contract=DriverLicense\n    )\n]\n\n# Classify document\nresult = process.classify(\"document.pdf\", classifications)\n</code></pre> <p>Benefits - High Performance</p> <ul> <li>Fast inference times</li> <li>State-of-the-art language models</li> </ul>"},{"location":"examples/local-processing/","title":"Local Processing with Ollama and Tesseract","text":"<p>This guide demonstrates how to process documents locally using Tesseract OCR and Ollama.</p>"},{"location":"examples/local-processing/#basic-setup","title":"Basic Setup","text":"<p>Here's how to use Tesseract with Ollama:</p> <pre><code>from extract_thinker import Extractor, Contract, LLM, DocumentLoaderTesseract\nfrom typing import List\nfrom pydantic import Field\n\nclass InvoiceContract(Contract):\n    invoice_number: str = Field(\"Invoice number\")\n    invoice_date: str = Field(\"Invoice date\")\n    total_amount: float = Field(\"Total amount\")\n    lines: List[LineItem] = Field(\"List of line items\")\n\n# Initialize Tesseract\nextractor = Extractor()\nextractor.load_document_loader(\n    DocumentLoaderTesseract(os.getenv(\"TESSERACT_PATH\"))\n)\n\nos.environ[\"API_BASE\"] = \"http://localhost:11434\"\n\n# Configure Ollama\nextractor.load_llm(\"ollama/phi3\")\n\n# Process document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n</code></pre> <p>Benefits - Privacy &amp; Security</p> <ul> <li>All processing done locally</li> <li>No data leaves your network</li> <li>Complete control over data</li> </ul>"},{"location":"examples/resume-processing/","title":"Resume Processing Example","text":"<p>ExtractThinker can process resumes and job descriptions, comparing requirements against candidate qualifications. This guide shows how to use ExtractThinker for automated resume screening and matching.</p>"},{"location":"examples/resume-processing/#basic-resume-processing","title":"Basic Resume Processing","text":"<p>Here's a complete example of processing job requirements and matching them against candidate resumes:</p> <pre><code>from extract_thinker import Extractor, Contract, DocumentLoaderPyPdf, LLM\nfrom typing import List, Optional\nfrom pydantic import Field\n\n# Define the job role contract\nclass RoleContract(Contract):\n    company_name: str = Field(\"Company name\")\n    years_of_experience: int = Field(\"Years of experience required. If not mention, calculate with start date and end date\")\n    is_remote: bool = Field(\"Is the role remote?\")\n    country: str = Field(\"Country of the role\")\n    city: Optional[str] = Field(\"City of the role\")\n    list_of_skills: List[str] = Field(\"\"\"\n        list of strings, e.g [\"5 years experience\", \"3 years in React\", \"Typescript\"]\n        Make the lists of skills to be a yes/no list for matching with candidates\n    \"\"\")\n\n# Define the resume contract\nclass ResumeContract(Contract):\n    name: str = Field(\"First and Last Name\")\n    age: Optional[str] = Field(\"Age with format DD/MM/YYYY. Empty if not available\")\n    email: str = Field(\"Email address\")\n    phone: Optional[str] = Field(\"Phone number\")\n    address: Optional[str] = Field(\"Address\")\n    city: Optional[str] = Field(\"City\")\n    total_experience: int = Field(\"Total experience in years\")\n    can_go_to_office: Optional[bool] = Field(\"Can go to office. If city/location is not provider, is false. If is the same city, is true\")\n    list_of_skills: List[bool] = Field(\"Takes the list of skills and returns a list of true/false, if the candidate has that skill\")\n</code></pre>"},{"location":"examples/resume-processing/#processing-job-requirements","title":"Processing Job Requirements","text":"<p>First, process the job requirements document:</p> <pre><code># Initialize extractor for job role\nextractor_job_role = Extractor()\nextractor_job_role.load_document_loader(DocumentLoaderPyPdf())\nextractor_job_role.load_llm(\"gpt-4o\")\n\n# Extract job requirements\nrole_result = extractor_job_role.extract(\n    \"Job_Offer.pdf\", \n    RoleContract\n)\n\n# Convert to YAML format for better readability\njob_role_content = \"Job Requirements:\\n\" + json_to_yaml(json.loads(role_result.json()))\n</code></pre>"},{"location":"examples/resume-processing/#processing-candidate-resumes","title":"Processing Candidate Resumes","text":"<p>Then process candidate resumes against the job requirements:</p> <pre><code># Initialize extractor for candidate resume\nextractor_candidate = Extractor()\nextractor_candidate.load_document_loader(DocumentLoaderPyPdf())\n\n# Configure LLM with Groq\nllm = LLM(\"groq/llama3-8b-8192\")  # default model\nextractor_candidate.load_llm(llm)\n\n# Process resume with job context\nresult = extractor_candidate.extract(\n    \"CV_Candidate.pdf\",\n    ResumeContract,\n    content=job_role_content  # Provide job requirements as context\n)\n</code></pre>"},{"location":"examples/resume-processing/#advanced-configuration-with-router","title":"Advanced Configuration with Router","text":"<p>For production environments, you can configure a router with multiple models and fallbacks:</p> <pre><code>def config_router():\n    rpm = 5000  # Rate limit in requests per minute\n\n    model_list = [\n        {\n            \"model_name\": \"Meta-Llama-3-8B-Instruct\",\n            \"litellm_params\": {\n                \"model\": \"deepinfra/meta-llama/Meta-Llama-3-8B-Instruct\",\n                \"api_key\": os.getenv(\"DEEPINFRA_API_KEY\"),\n                \"rpm\": rpm,\n            },\n        },\n        {\n            \"model_name\": \"Mistral-7B-Instruct-v0.2\",\n            \"litellm_params\": {\n                \"model\": \"deepinfra/mistralai/Mistral-7B-Instruct-v0.2\",\n                \"api_key\": os.getenv(\"DEEPINFRA_API_KEY\"),\n                \"rpm\": rpm,\n            }\n        },\n        {\n            \"model_name\": \"groq-llama3-8b-8192\",\n            \"litellm_params\": {\n                \"model\": \"groq/llama3-8b-8192\",\n                \"api_key\": os.getenv(\"GROQ_API_KEY\"),\n                \"rpm\": rpm,\n            }\n        },\n    ]\n\n    # Add fallback models\n    fallback_models = [\n        {\n            \"model_name\": \"claude-3-haiku-20240307\",\n            \"litellm_params\": {\n                \"model\": \"claude-3-haiku-20240307\",\n                \"api_key\": os.getenv(\"CLAUDE_API_KEY\"),\n            }\n        }\n    ]\n\n    router = Router(\n        model_list=model_list + fallback_models,\n        default_fallbacks=[\"claude-3-haiku-20240307\"],\n        context_window_fallbacks=[\n            {\"Meta-Llama-3-8B-Instruct\": [\"claude-3-haiku-20240307\"]},\n            {\"groq-llama3-8b-8192\": [\"claude-3-haiku-20240307\"]},\n            {\"Mistral-7B-Instruct-v0.2\": [\"claude-3-haiku-20240307\"]}\n        ],\n        set_verbose=True\n    )\n\n    return router\n</code></pre>"},{"location":"examples/resume-processing/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Automated resume screening</li> <li>Skill matching against job requirements</li> <li>Experience verification</li> <li>Location compatibility checking</li> <li>Batch processing of multiple resumes</li> </ul>"},{"location":"getting-started/","title":"Extract Thinker","text":""},{"location":"getting-started/#the-first-framework-for-document-intelligence-processing-idp-for-llms","title":"The first Framework for Document Intelligence Processing (IDP) - for LLMs","text":"\u2605 Star the Repo Examples Production Workflows <p>Is a flexible document intelligence framework that helps you extract and classify structured data from various documents, acting like an ORM for document processing workflows. One phrase you say is \u201cDocument Intelligence for LLMs\u201d or \u201cLangChain for Intelligent Document Processing.\u201d The motivation is to create niche features required for document processing, like splitting large documents and advanced classification.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install using pip:</p> <pre><code>pip install extract_thinker\n</code></pre>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":"<p>Here's a simple example that extracts invoice data from a PDF:</p> <pre><code>from extract_thinker import Extractor, DocumentLoaderPyPdf, Contract\n\n# Define what data you want to extract\nclass InvoiceContract(Contract):\n    invoice_number: str\n    invoice_date: str\n    total_amount: float\n\n# Initialize the extractor\nextractor = Extractor()\nextractor.load_document_loader(DocumentLoaderPyPdf())\nextractor.load_llm(\"gpt-4\")  # or any other supported model\n\n# Extract data from your document\nresult = extractor.extract(\"invoice.pdf\", InvoiceContract)\n\nprint(f\"Invoice #{result.invoice_number}\")\nprint(f\"Date: {result.invoice_date}\")\nprint(f\"Total: ${result.total_amount}\")\n</code></pre>"},{"location":"getting-started/#native-features-that-you-want","title":"Native Features that you want","text":"<ul> <li> <p> Extraction with Pydantic</p> <p>Extract structured data from any document type using Pydantic models for validation, custom features, and prompt engineering capabilities.</p> </li> <li> <p> Classification &amp; Split</p> <p>Intelligent document classification and splitting with support for consensus strategies, eager/lazy splitting, and confidence thresholds.</p> </li> <li> <p> PII Detection</p> <p>Automatically detect and handle sensitive personal information in documents with privacy-first approach and advanced validation.</p> </li> <li> <p> LLM and OCR Agnostic</p> <p>Freedom to choose and switch between different LLM providers and OCR engines based on your needs and cost requirements.</p> </li> </ul>"}]}